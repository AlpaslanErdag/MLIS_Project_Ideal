{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "E_HVCxZiW8E8",
        "MhCmi37GYpRx",
        "Yy9l9puJZHHX",
        "O8piuV5rZUtD",
        "a_7VxHj6bvEf",
        "LzAWM5XFejHV",
        "9ZR6i_Qef-4x",
        "IVFmsUW3stPb",
        "CgkNsFilIy0F"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyORMarYxUgkJTGCV+UYTMwY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anshuman-37/MLIS_Project_Ideal/blob/main/Logistic_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading data"
      ],
      "metadata": {
        "id": "Uww_zAIyC6F1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rPgBnpUuCYxo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xlrd\n",
        "!pip install openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4UuMENmceho",
        "outputId": "3a5cd77d-0209-4662-9783-7dab35a559f3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (2.5.9)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.4.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"breast-cancer-wisconsin.names\") as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4hhSfAecg38",
        "outputId": "05e95f59-1426-4895-8da6-4a866cd6ccdc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Citation Request:\n",
            "   This breast cancer databases was obtained from the University of Wisconsin\n",
            "   Hospitals, Madison from Dr. William H. Wolberg.  If you publish results\n",
            "   when using this database, then please include this information in your\n",
            "   acknowledgements.  Also, please cite one or more of:\n",
            "\n",
            "   1. O. L. Mangasarian and W. H. Wolberg: \"Cancer diagnosis via linear \n",
            "      programming\", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.\n",
            "\n",
            "   2. William H. Wolberg and O.L. Mangasarian: \"Multisurface method of \n",
            "      pattern separation for medical diagnosis applied to breast cytology\", \n",
            "      Proceedings of the National Academy of Sciences, U.S.A., Volume 87, \n",
            "      December 1990, pp 9193-9196.\n",
            "\n",
            "   3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg: \"Pattern recognition \n",
            "      via linear programming: Theory and application to medical diagnosis\", \n",
            "      in: \"Large-scale numerical optimization\", Thomas F. Coleman and Yuying\n",
            "      Li, editors, SIAM Publications, Philadelphia 1990, pp 22-30.\n",
            "\n",
            "   4. K. P. Bennett & O. L. Mangasarian: \"Robust linear programming \n",
            "      discrimination of two linearly inseparable sets\", Optimization Methods\n",
            "      and Software 1, 1992, 23-34 (Gordon & Breach Science Publishers).\n",
            "\n",
            "1. Title: Wisconsin Breast Cancer Database (January 8, 1991)\n",
            "\n",
            "2. Sources:\n",
            "   -- Dr. WIlliam H. Wolberg (physician)\n",
            "      University of Wisconsin Hospitals\n",
            "      Madison, Wisconsin\n",
            "      USA\n",
            "   -- Donor: Olvi Mangasarian (mangasarian@cs.wisc.edu)\n",
            "      Received by David W. Aha (aha@cs.jhu.edu)\n",
            "   -- Date: 15 July 1992\n",
            "\n",
            "3. Past Usage:\n",
            "\n",
            "   Attributes 2 through 10 have been used to represent instances.\n",
            "   Each instance has one of 2 possible classes: benign or malignant.\n",
            "\n",
            "   1. Wolberg,~W.~H., \\& Mangasarian,~O.~L. (1990). Multisurface method of \n",
            "      pattern separation for medical diagnosis applied to breast cytology. In\n",
            "      {\\it Proceedings of the National Academy of Sciences}, {\\it 87},\n",
            "      9193--9196.\n",
            "      -- Size of data set: only 369 instances (at that point in time)\n",
            "      -- Collected classification results: 1 trial only\n",
            "      -- Two pairs of parallel hyperplanes were found to be consistent with\n",
            "         50% of the data\n",
            "         -- Accuracy on remaining 50% of dataset: 93.5%\n",
            "      -- Three pairs of parallel hyperplanes were found to be consistent with\n",
            "         67% of data\n",
            "         -- Accuracy on remaining 33% of dataset: 95.9%\n",
            "\n",
            "   2. Zhang,~J. (1992). Selecting typical instances in instance-based\n",
            "      learning.  In {\\it Proceedings of the Ninth International Machine\n",
            "      Learning Conference} (pp. 470--479).  Aberdeen, Scotland: Morgan\n",
            "      Kaufmann.\n",
            "      -- Size of data set: only 369 instances (at that point in time)\n",
            "      -- Applied 4 instance-based learning algorithms \n",
            "      -- Collected classification results averaged over 10 trials\n",
            "      -- Best accuracy result: \n",
            "         -- 1-nearest neighbor: 93.7%\n",
            "         -- trained on 200 instances, tested on the other 169\n",
            "      -- Also of interest:\n",
            "         -- Using only typical instances: 92.2% (storing only 23.1 instances)\n",
            "         -- trained on 200 instances, tested on the other 169\n",
            "\n",
            "4. Relevant Information:\n",
            "\n",
            "   Samples arrive periodically as Dr. Wolberg reports his clinical cases.\n",
            "   The database therefore reflects this chronological grouping of the data.\n",
            "   This grouping information appears immediately below, having been removed\n",
            "   from the data itself:\n",
            "\n",
            "     Group 1: 367 instances (January 1989)\n",
            "     Group 2:  70 instances (October 1989)\n",
            "     Group 3:  31 instances (February 1990)\n",
            "     Group 4:  17 instances (April 1990)\n",
            "     Group 5:  48 instances (August 1990)\n",
            "     Group 6:  49 instances (Updated January 1991)\n",
            "     Group 7:  31 instances (June 1991)\n",
            "     Group 8:  86 instances (November 1991)\n",
            "     -----------------------------------------\n",
            "     Total:   699 points (as of the donated datbase on 15 July 1992)\n",
            "\n",
            "   Note that the results summarized above in Past Usage refer to a dataset\n",
            "   of size 369, while Group 1 has only 367 instances.  This is because it\n",
            "   originally contained 369 instances; 2 were removed.  The following\n",
            "   statements summarizes changes to the original Group 1's set of data:\n",
            "\n",
            "   #####  Group 1 : 367 points: 200B 167M (January 1989)\n",
            "   #####  Revised Jan 10, 1991: Replaced zero bare nuclei in 1080185 & 1187805\n",
            "   #####  Revised Nov 22,1991: Removed 765878,4,5,9,7,10,10,10,3,8,1 no record\n",
            "   #####                  : Removed 484201,2,7,8,8,4,3,10,3,4,1 zero epithelial\n",
            "   #####                  : Changed 0 to 1 in field 6 of sample 1219406\n",
            "   #####                  : Changed 0 to 1 in field 8 of following sample:\n",
            "   #####                  : 1182404,2,3,1,1,1,2,0,1,1,1\n",
            "\n",
            "5. Number of Instances: 699 (as of 15 July 1992)\n",
            "\n",
            "6. Number of Attributes: 10 plus the class attribute\n",
            "\n",
            "7. Attribute Information: (class attribute has been moved to last column)\n",
            "\n",
            "   #  Attribute                     Domain\n",
            "   -- -----------------------------------------\n",
            "   1. Sample code number            id number\n",
            "   2. Clump Thickness               1 - 10\n",
            "   3. Uniformity of Cell Size       1 - 10\n",
            "   4. Uniformity of Cell Shape      1 - 10\n",
            "   5. Marginal Adhesion             1 - 10\n",
            "   6. Single Epithelial Cell Size   1 - 10\n",
            "   7. Bare Nuclei                   1 - 10\n",
            "   8. Bland Chromatin               1 - 10\n",
            "   9. Normal Nucleoli               1 - 10\n",
            "  10. Mitoses                       1 - 10\n",
            "  11. Class:                        (2 for benign, 4 for malignant)\n",
            "\n",
            "8. Missing attribute values: 16\n",
            "\n",
            "   There are 16 instances in Groups 1 to 6 that contain a single missing \n",
            "   (i.e., unavailable) attribute value, now denoted by \"?\".  \n",
            "\n",
            "9. Class distribution:\n",
            " \n",
            "   Benign: 458 (65.5%)\n",
            "   Malignant: 241 (34.5%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yixin's Data Formating \n",
        "    It will all collapse in a single place"
      ],
      "metadata": {
        "id": "kjUJDI5GclMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_name = ['Samplecodenumber','ClumpThickness','UniformityofCellSize','UniformityofCellShape',\n",
        "            'MarginalAdhesion','SingleEpithelialCellSize','BareNuclei',\n",
        "            'BlandChromatin','NormalNucleoli','Mitoses','Class']\n",
        "cancerdata = pd.read_csv('/content/tumor.csv', low_memory=False,names=col_name)\n",
        "cancerdata.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "XmFtvpCrclgD",
        "outputId": "9881e240-d686-4e96-bc02-a2257c153f83"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-09e24545-8f07-4102-ba7f-92e0dff74b34\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Samplecodenumber</th>\n",
              "      <th>ClumpThickness</th>\n",
              "      <th>UniformityofCellSize</th>\n",
              "      <th>UniformityofCellShape</th>\n",
              "      <th>MarginalAdhesion</th>\n",
              "      <th>SingleEpithelialCellSize</th>\n",
              "      <th>BareNuclei</th>\n",
              "      <th>BlandChromatin</th>\n",
              "      <th>NormalNucleoli</th>\n",
              "      <th>Mitoses</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sample code number</td>\n",
              "      <td>Clump Thickness</td>\n",
              "      <td>Uniformity of Cell Size</td>\n",
              "      <td>Uniformity of Cell Shape</td>\n",
              "      <td>Marginal Adhesion</td>\n",
              "      <td>Single Epithelial Cell Size</td>\n",
              "      <td>Bare Nuclei</td>\n",
              "      <td>Bland Chromatin</td>\n",
              "      <td>Normal Nucleoli</td>\n",
              "      <td>Mitoses</td>\n",
              "      <td>Class</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000025</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1002945</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1015425</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1016277</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09e24545-8f07-4102-ba7f-92e0dff74b34')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-09e24545-8f07-4102-ba7f-92e0dff74b34 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-09e24545-8f07-4102-ba7f-92e0dff74b34');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     Samplecodenumber   ClumpThickness  ...  Mitoses  Class\n",
              "0  Sample code number  Clump Thickness  ...  Mitoses  Class\n",
              "1             1000025                5  ...        1      2\n",
              "2             1002945                5  ...        1      2\n",
              "3             1015425                3  ...        1      2\n",
              "4             1016277                6  ...        1      2\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop the column that represent ID\n",
        "cancerdata = cancerdata.drop(['Samplecodenumber'], 1)\n",
        "# Check the loan status and distinct the target value.\n",
        "cancerdata['Class'].value_counts()\n",
        "#Select the finished loan including repaid and late, delete the 'current' loan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_J6Oam5c7vP",
        "outputId": "34258e60-ee31-4b5c-da9f-5b5879a78b96"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2        444\n",
              "4        239\n",
              "Class      1\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Select the finished loan including repaid and late, delete the 'current' loan\n",
        "Benign = cancerdata[(cancerdata.Class == 2) ].sample(240).index\n",
        "Malignant = cancerdata[(cancerdata.Class == 4) ].sample(240).index\n",
        "cancer = cancerdata.loc[Benign|Malignant]\n",
        "cancer = cancer.reset_index(drop=True)\n",
        "cancer['classes'] = cancer.Class.map({2:0,4:1})\n",
        "cancer = cancer.drop(['Class'], 1)\n",
        "cancer.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "xGRuBeaUdxH6",
        "outputId": "d2d20f37-6fbe-474d-96de-16d133a06bd4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-102ad8c86437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Select the finished loan including repaid and late, delete the 'current' loan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mBenign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancerdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcancerdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClass\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m240\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mMalignant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancerdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcancerdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClass\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m240\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcancer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancerdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBenign\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0mMalignant\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcancer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis)\u001b[0m\n\u001b[1;32m   4993\u001b[0m             )\n\u001b[1;32m   4994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4995\u001b[0;31m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4996\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: a must be greater than 0 unless no samples are taken"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/tumor.csv')"
      ],
      "metadata": {
        "id": "3_zd1JTRCiRM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haiRAO59C1C0",
        "outputId": "eff76389-441f-4cd2-81b8-8a345c882354"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 683 entries, 0 to 682\n",
            "Data columns (total 11 columns):\n",
            " #   Column                       Non-Null Count  Dtype\n",
            "---  ------                       --------------  -----\n",
            " 0   Sample code number           683 non-null    int64\n",
            " 1   Clump Thickness              683 non-null    int64\n",
            " 2   Uniformity of Cell Size      683 non-null    int64\n",
            " 3   Uniformity of Cell Shape     683 non-null    int64\n",
            " 4   Marginal Adhesion            683 non-null    int64\n",
            " 5   Single Epithelial Cell Size  683 non-null    int64\n",
            " 6   Bare Nuclei                  683 non-null    int64\n",
            " 7   Bland Chromatin              683 non-null    int64\n",
            " 8   Normal Nucleoli              683 non-null    int64\n",
            " 9   Mitoses                      683 non-null    int64\n",
            " 10  Class                        683 non-null    int64\n",
            "dtypes: int64(11)\n",
            "memory usage: 58.8 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a classifier"
      ],
      "metadata": {
        "id": "KF62ewDEDA_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting X & Y\n"
      ],
      "metadata": {
        "id": "E_HVCxZiW8E8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO \n",
        "# have to randomize the data first\n",
        "# After that have to divide in X and Y \n",
        "\n",
        "## Target Values \n",
        "X = df.drop(columns=['Class'])\n",
        "X = np.array(X)\n",
        "print(X.shape)\n",
        "\n",
        "## Label Values\n",
        "Y = df['Class']\n",
        "Y = np.array(Y)\n",
        "print(Y.shape)\n",
        "\n",
        "# for i in range(0,len(Y)):\n",
        "#     if Y[i] == 4:\n",
        "#         Y[i] = 1\n",
        "#     else:\n",
        "#         Y[i]=0\n",
        "# # Storing the number of rows and columns in X\n",
        "# rows , cols = X.shape\n",
        "## Making 2 and 4 to 0 and 1 to get result according to sigmoid function\n",
        "\n",
        "Y = list(map(lambda x : 1 if x==2 else 0, Y))\n",
        "Y = np.array(Y)\n",
        "\n",
        "# X_train = X[0:513]\n",
        "# X_test = X[513:]\n",
        "# Y_train = Y[0:513]\n",
        "# Y_test = Y[513:]\n",
        "# print(X_train.shape,Y_train.shape)\n",
        "# print(X_test.shape,Y_test.shape)\n",
        "\n",
        "#This normalization doesnot work \n",
        "\n",
        "#X=(X-X.mean())/X.std()\n",
        "\n",
        "# X=(X-X.min())/(X.max()-X.min())\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=15)\n",
        "\n",
        "\n",
        "# Using the inbuilt one gives 100% accuracy because it is dumbdata \n",
        "#Standardizing the data.\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGU0KqqjW8Xw",
        "outputId": "d4430408-49c8-4144-9b77-0e53102a0fe7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(683, 10)\n",
            "(683,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((512, 10), (512,), (171, 10), (171,))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intialize the weights"
      ],
      "metadata": {
        "id": "MhCmi37GYpRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(dim):\n",
        "    ''' In this function, we will initialize our weights and bias'''\n",
        "    # Here dimenstion refer to the number of the attributes in the data\n",
        "    w = np.zeros(shape=len(dim))\n",
        "    b = 0\n",
        "    return w,b"
      ],
      "metadata": {
        "id": "pRqb-07xDD6m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Just to check wether the function is working fine\n",
        "dim=X_train[0]\n",
        "w,b = initialize_weights(dim)\n",
        "print('w =',(w))\n",
        "print('b =',b)\n",
        "print('w',np.sum(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z96iV0wuDD9Y",
        "outputId": "a5056957-03d8-4262-dfd5-4d9393fa85e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "b = 0\n",
            "w 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grader Function - Weights\n",
        "    Check whether things are working fine or not"
      ],
      "metadata": {
        "id": "Yy9l9puJZHHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Grader Function\n",
        "## Dont run this cell untill and unless you want to check wether everything is fine or not \n",
        "w,b = initialize_weights(dim)\n",
        "def grader_weights(w,b):\n",
        "  assert((len(w)==len(dim)) and b==0 and np.sum(w)==0.0)\n",
        "  return True\n",
        "grader_weights(w,b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Nzv0I31DEAT",
        "outputId": "64da6018-caad-4d75-e9c7-f1302f0c194d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Sigmoid\n",
        "\n",
        "![Sigmoid Function](https://www.gstatic.com/education/formulas2/397133473/en/sigmoid_function.svg)"
      ],
      "metadata": {
        "id": "O8piuV5rZUtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    ''' In this function, we will return sigmoid of z'''\n",
        "    # compute sigmoid(z) and return\n",
        "    return 1/(1+np.exp(-z))"
      ],
      "metadata": {
        "id": "CWYiNX80ZUCS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Log Loss \n",
        "![Log Loss](https://miro.medium.com/max/1192/1*wilGXrItaMAJmZNl6RJq9Q.png)"
      ],
      "metadata": {
        "id": "a_7VxHj6bvEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(y_labels,y_predicted):\n",
        "    '''This function will return the log loss of the function'''\n",
        "    loss = -1 * (np.sum((y_labels * np.log10(y_predicted))+ \\\n",
        "                      ((1-y_labels)*np.log10(1-y_predicted))))/len(y_labels)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "UAnfSyDrDECM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grader Function log_loss"
      ],
      "metadata": {
        "id": "LzAWM5XFejHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grader_logloss(true,pred):\n",
        "    loss=log_loss(true,pred)\n",
        "    assert(loss==0.07644900402910389)\n",
        "    return True\n",
        "true=np.array([1,1,0,1,0])\n",
        "pred=np.array([0.9,0.8,0.1,0.8,0.2])\n",
        "grader_logloss(true,pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emvDMysqDEE9",
        "outputId": "1368a5ff-f72f-4ea1-9cda-1d54a394b212"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient with respect w (dw)\n",
        "![Differntiation of cost function wrt to w](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-35e8bb42947bd888580c2a8a9fe8fe0e_l3.svg)"
      ],
      "metadata": {
        "id": "9ZR6i_Qef-4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_dw(x,y,w,b,alpha,N):\n",
        "    '''In this function, we will compute the gardient w.r.to w '''\n",
        "    # Calculcating the graindent of weighted vectors\n",
        "    return x * (y - sigmoid(np.dot(w, x) + b)) - alpha/N*w"
      ],
      "metadata": {
        "id": "_haqFGTcgAg-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grader_dw(x,y,w,b,alpha,N):\n",
        "    grad_dw=gradient_dw(x,y,w,b,alpha,N)\n",
        "    assert(np.sum(grad_dw)==2.7259648199999997)\n",
        "    return True\n",
        "\n",
        "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
        "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725])\n",
        "\n",
        "grad_y=0\n",
        "grad_w,grad_b=initialize_weights(dim)\n",
        "alpha=0.0001\n",
        "N=len(X)\n",
        "grader_dw(grad_x,grad_y,grad_w,grad_b,alpha,N)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c9X9RsKgBkg",
        "outputId": "a258d8b1-b164-4966-82be-2b62290f183d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute gradient w.r.t 'b'"
      ],
      "metadata": {
        "id": "IVFmsUW3stPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_db(x,y,w,b):\n",
        "    '''In this function, we will compute gradient w.r.to b '''\n",
        "    # Calculating the gradient of bais\n",
        "    return y - sigmoid(np.dot(w, x) + b)"
      ],
      "metadata": {
        "id": "AERrDcGGstfy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grader_db(x,y,w,b):\n",
        "  grad_db=gradient_db(x,y,w,b)\n",
        "  assert(grad_db==-0.5)\n",
        "  return True\n",
        "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
        "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725])\n",
        "grad_y=0\n",
        "grad_w,grad_b=initialize_weights(grad_x)\n",
        "alpha=0.0001\n",
        "N=len(X_train)\n",
        "grader_db(grad_x,grad_y,grad_w,grad_b)\n",
        "True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9csvJwjHWtF",
        "outputId": "fb29e394-26ed-4886-d7fd-cad71d383752"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Function"
      ],
      "metadata": {
        "id": "CgkNsFilIy0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0,p):\n",
        "    ''' In this function, we will implement logistic regression'''\n",
        "    \n",
        "    w,b = initialize_weights(X_train[0]) # intilize weight vectors\n",
        "    same_loss_counter = 0\n",
        "    N = len(X_train)\n",
        "    train_loss , test_loss = [],[]\n",
        "    part_no = 0\n",
        "    part_size = 25\n",
        "    ctr = 0\n",
        "    n = len(X_train)\n",
        "    \n",
        "    # Loop to traveres in epoches\n",
        "    for i in tqdm(range(0,epochs)):\n",
        "        # Loop to access data point in the \n",
        "        for j in range(part_size):\n",
        "            \n",
        "            # Calculating gradient of w and adding it to the existing one    \n",
        "            w = w + eta0*gradient_dw(X_train[(j+part_no)%n], y_train[(j+part_no)%n],w, b, alpha, len(X_train))\n",
        "            \n",
        "            #Calculating gradient of b and adding it to the existing one\n",
        "            b = b + eta0*gradient_db(X_train[(j+part_no)%n], y_train[(j+part_no)%n], w, b)\n",
        "        \n",
        "\n",
        "        part_no = (part_no + part_size)%n # To updtae the new part\n",
        "\n",
        "        #Predicting the traing data in comparison of the the xtrain\n",
        "        y_pred_train = np.array([sigmoid(np.dot(w, x)+b) for x in X_train])\n",
        "        \n",
        "        #Predicting the test data in comaprison of the xtest\n",
        "        y_pred_test = np.array([sigmoid(np.dot(w, x)+b) for x in X_test])\n",
        "\n",
        "        #Calculating the loss on for training data\n",
        "        loss = log_loss(y_train,y_pred_train)\n",
        "        train_loss.append(loss)\n",
        "        \n",
        "        #Calculatig the loss onfor testing data\n",
        "        loss = log_loss(y_test,y_pred_test)\n",
        "        test_loss.append(loss)\n",
        "\n",
        "        ## Printing values\n",
        "        print('\\n-- Epoch no(iteration no) ', i+1,'\\n Train data set : ')\n",
        "        #print('Actual values: ', y_train ,'\\n Predicted Values : ', y_pred_train)\n",
        "        #print('Test data set :') \n",
        "       # print('Actual values: ', y_test, '\\nPredicated Values : ', y_pred_test)\n",
        "        print('W intercept: {}, B intercept: {}, Train loss: {}, Test loss: {}'\\\n",
        "              .format(w, b, train_loss[i], test_loss[i]))\n",
        "    return w,b,train_loss,test_loss"
      ],
      "metadata": {
        "id": "2aLSyLZRHKun"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha=0.0001\n",
        "eta0=0.01\n",
        "N=len(X_train)\n",
        "epochs=100\n",
        "p = 2\n",
        "w,b,train_loss,test_loss=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0,p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V56xhKCJfbA",
        "outputId": "02e4aad3-3443-47bd-d3b0-c6f3cb605d75"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Epoch no(iteration no)  1 \n",
            " Train data set : \n",
            "W intercept: [ 0.0257477  -0.04207267 -0.08446602 -0.08226939 -0.10460032 -0.07926888\n",
            " -0.09616748 -0.09249272 -0.07438199 -0.02683102], B intercept: 0.0425824621900922, Train loss: 0.2086441138565775, Test loss: 0.212036374848482\n",
            "\n",
            "-- Epoch no(iteration no)  2 \n",
            " Train data set : \n",
            "W intercept: [ 0.01522765 -0.10404316 -0.15321883 -0.13852272 -0.1602512  -0.13586019\n",
            " -0.15835717 -0.15144124 -0.11967582 -0.06012878], B intercept: 0.07772201936375873, Train loss: 0.16139947695007967, Test loss: 0.16551252573718417\n",
            "\n",
            "-- Epoch no(iteration no)  3 \n",
            " Train data set : \n",
            "W intercept: [ 0.032101   -0.13232912 -0.19662372 -0.1886326  -0.19437023 -0.17353812\n",
            " -0.20806407 -0.17945805 -0.14599034 -0.0758225 ], B intercept: 0.11107860760244236, Train loss: 0.1370472692862048, Test loss: 0.14131045267327985\n",
            "\n",
            "-- Epoch no(iteration no)  4 \n",
            " Train data set : \n",
            "W intercept: [ 0.0298703  -0.16602074 -0.22649546 -0.22268998 -0.21803233 -0.1931118\n",
            " -0.26617445 -0.21316415 -0.16953355 -0.0837867 ], B intercept: 0.11271831897254825, Train loss: 0.12097800481193768, Test loss: 0.12472300091057538\n",
            "\n",
            "-- Epoch no(iteration no)  5 \n",
            " Train data set : \n",
            "W intercept: [ 0.03002683 -0.20061127 -0.26706002 -0.2585516  -0.24001181 -0.21903083\n",
            " -0.29986105 -0.23985722 -0.1975086  -0.10201682], B intercept: 0.14441124845239034, Train loss: 0.10645993388828658, Test loss: 0.11003656377540753\n",
            "\n",
            "-- Epoch no(iteration no)  6 \n",
            " Train data set : \n",
            "W intercept: [ 0.0345257  -0.22737179 -0.29288355 -0.28745807 -0.26215081 -0.25020143\n",
            " -0.33714524 -0.25761229 -0.22370296 -0.11639681], B intercept: 0.17539459991675793, Train loss: 0.0955330484461028, Test loss: 0.09906480950717705\n",
            "\n",
            "-- Epoch no(iteration no)  7 \n",
            " Train data set : \n",
            "W intercept: [ 0.03903375 -0.2657433  -0.31556086 -0.31599831 -0.28756192 -0.27001913\n",
            " -0.36360107 -0.27431287 -0.24726678 -0.13071963], B intercept: 0.1900741416830398, Train loss: 0.08724071227718795, Test loss: 0.09033179765317537\n",
            "\n",
            "-- Epoch no(iteration no)  8 \n",
            " Train data set : \n",
            "W intercept: [ 0.044899   -0.28063838 -0.33491672 -0.33208379 -0.29843366 -0.28820804\n",
            " -0.38441073 -0.28582117 -0.26510344 -0.14803587], B intercept: 0.21288808354538952, Train loss: 0.08189082911052156, Test loss: 0.08488710191216033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 15/100 [00:00<00:00, 144.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Epoch no(iteration no)  9 \n",
            " Train data set : \n",
            "W intercept: [ 0.06156538 -0.28873054 -0.35786708 -0.35019632 -0.32486698 -0.29275898\n",
            " -0.41385184 -0.30020356 -0.28674031 -0.15499434], B intercept: 0.22594079974166162, Train loss: 0.07688221138393461, Test loss: 0.07981817348305616\n",
            "\n",
            "-- Epoch no(iteration no)  10 \n",
            " Train data set : \n",
            "W intercept: [ 0.06176645 -0.31207578 -0.37753156 -0.37005503 -0.33970868 -0.31115731\n",
            " -0.43158755 -0.31811846 -0.31132788 -0.16097325], B intercept: 0.23810314345338426, Train loss: 0.07234481087286848, Test loss: 0.07500268909660301\n",
            "\n",
            "-- Epoch no(iteration no)  11 \n",
            " Train data set : \n",
            "W intercept: [ 0.06377187 -0.31879798 -0.37813341 -0.37259586 -0.34883933 -0.32852232\n",
            " -0.4401724  -0.33732866 -0.3058134  -0.16100961], B intercept: 0.2665515750622034, Train loss: 0.07017514583961136, Test loss: 0.0729903647882988\n",
            "\n",
            "-- Epoch no(iteration no)  12 \n",
            " Train data set : \n",
            "W intercept: [ 0.06370066 -0.32814287 -0.39188353 -0.38573918 -0.36478614 -0.33754835\n",
            " -0.4590652  -0.35582734 -0.32400665 -0.17031459], B intercept: 0.28292515836814697, Train loss: 0.0669636731083351, Test loss: 0.06967079229472346\n",
            "\n",
            "-- Epoch no(iteration no)  13 \n",
            " Train data set : \n",
            "W intercept: [ 0.06389626 -0.34987331 -0.39995941 -0.39567645 -0.37571943 -0.35366702\n",
            " -0.47719954 -0.37187276 -0.33959051 -0.17505041], B intercept: 0.29311044977854367, Train loss: 0.06424160768289873, Test loss: 0.0666944143648164\n",
            "\n",
            "-- Epoch no(iteration no)  14 \n",
            " Train data set : \n",
            "W intercept: [ 0.07304564 -0.35615316 -0.41565599 -0.41188752 -0.38649147 -0.36578\n",
            " -0.49772204 -0.38573578 -0.34738934 -0.17837272], B intercept: 0.3016271240611623, Train loss: 0.06195795321788501, Test loss: 0.06429219270043177\n",
            "\n",
            "-- Epoch no(iteration no)  15 \n",
            " Train data set : \n",
            "W intercept: [ 0.07373119 -0.36912801 -0.42817741 -0.42302606 -0.39713118 -0.37547591\n",
            " -0.51221088 -0.40324141 -0.36059619 -0.18286584], B intercept: 0.31789580937508594, Train loss: 0.05974771231746523, Test loss: 0.061941456670592324\n",
            "\n",
            "-- Epoch no(iteration no)  16 \n",
            " Train data set : \n",
            "W intercept: [ 0.07977882 -0.38665209 -0.43844091 -0.43721441 -0.40606793 -0.38212146\n",
            " -0.52748568 -0.41543635 -0.37862362 -0.19185627], B intercept: 0.3236032171039205, Train loss: 0.0578233924323919, Test loss: 0.05974032878523326\n",
            "\n",
            "-- Epoch no(iteration no)  17 \n",
            " Train data set : \n",
            "W intercept: [ 0.0782285  -0.39117256 -0.45250469 -0.45427952 -0.41666741 -0.39315464\n",
            " -0.5430916  -0.42923021 -0.38632842 -0.19488188], B intercept: 0.3368915356718118, Train loss: 0.05610100278518242, Test loss: 0.05794032172109598\n",
            "\n",
            "-- Epoch no(iteration no)  18 \n",
            " Train data set : \n",
            "W intercept: [ 0.08075005 -0.40194766 -0.43962114 -0.45016155 -0.4165338  -0.39200962\n",
            " -0.54856321 -0.42969787 -0.39089935 -0.20574587], B intercept: 0.35522982255473495, Train loss: 0.05559025866370049, Test loss: 0.05740100910187056\n",
            "\n",
            "-- Epoch no(iteration no)  19 \n",
            " Train data set : \n",
            "W intercept: [ 0.08158456 -0.41399343 -0.45123902 -0.46188698 -0.42537801 -0.39798448\n",
            " -0.56281728 -0.43360644 -0.39951343 -0.21181064], B intercept: 0.3667465966018321, Train loss: 0.05422770083412384, Test loss: 0.05589710948558267\n",
            "\n",
            "-- Epoch no(iteration no)  20 \n",
            " Train data set : \n",
            "W intercept: [ 0.0719204  -0.4287054  -0.45565487 -0.46772537 -0.42439003 -0.38213013\n",
            " -0.55770833 -0.43560237 -0.41107805 -0.2201215 ], B intercept: 0.3790594970793249, Train loss: 0.05372535748722862, Test loss: 0.05520892796902073\n",
            "\n",
            "-- Epoch no(iteration no)  21 \n",
            " Train data set : \n",
            "W intercept: [ 0.07525745 -0.43291638 -0.46623884 -0.47863469 -0.43280793 -0.3893283\n",
            " -0.56893132 -0.44575303 -0.42002545 -0.2235649 ], B intercept: 0.39210775587167174, Train loss: 0.052557966434658786, Test loss: 0.05398429662984798\n",
            "\n",
            "-- Epoch no(iteration no)  22 \n",
            " Train data set : \n",
            "W intercept: [ 0.07035461 -0.43437621 -0.47688268 -0.48342638 -0.44681922 -0.39373508\n",
            " -0.57494865 -0.45196631 -0.41692795 -0.22902407], B intercept: 0.40579103540702427, Train loss: 0.051775743246436676, Test loss: 0.05319991933469596\n",
            "\n",
            "-- Epoch no(iteration no)  23 \n",
            " Train data set : \n",
            "W intercept: [ 0.07501078 -0.42846122 -0.48300437 -0.4893258  -0.44560198 -0.3971538\n",
            " -0.57472725 -0.44462391 -0.40811048 -0.22946783], B intercept: 0.4247930131989541, Train loss: 0.05158043378024366, Test loss: 0.05317317166046045\n",
            "\n",
            "-- Epoch no(iteration no)  24 \n",
            " Train data set : \n",
            "W intercept: [ 0.07688905 -0.43725407 -0.4914261  -0.50052716 -0.45103386 -0.40712744\n",
            " -0.59167502 -0.45346162 -0.41788698 -0.23090621], B intercept: 0.42379704934229784, Train loss: 0.05063518809097956, Test loss: 0.052045191568320874\n",
            "\n",
            "-- Epoch no(iteration no)  25 \n",
            " Train data set : \n",
            "W intercept: [ 0.07499003 -0.44619422 -0.49477115 -0.50214322 -0.45823483 -0.39150618\n",
            " -0.60919022 -0.45871919 -0.40731355 -0.23028799], B intercept: 0.4220408621804005, Train loss: 0.050301295751139126, Test loss: 0.05157326010216334\n",
            "\n",
            "-- Epoch no(iteration no)  26 \n",
            " Train data set : \n",
            "W intercept: [ 0.07575169 -0.45682747 -0.50577347 -0.51233466 -0.46274967 -0.40415838\n",
            " -0.62384962 -0.46030207 -0.41301895 -0.23673892], B intercept: 0.4288156935959381, Train loss: 0.04939233970002736, Test loss: 0.05050730263773409\n",
            "\n",
            "-- Epoch no(iteration no)  27 \n",
            " Train data set : \n",
            "W intercept: [ 0.07594508 -0.47095889 -0.51233438 -0.52111014 -0.47160543 -0.40879273\n",
            " -0.63919577 -0.46588086 -0.42330866 -0.24032553], B intercept: 0.4304784139634069, Train loss: 0.048560689359753856, Test loss: 0.04947939201857941\n",
            "\n",
            "-- Epoch no(iteration no)  28 \n",
            " Train data set : \n",
            "W intercept: [ 0.08072552 -0.47213109 -0.51783911 -0.52330364 -0.47744866 -0.41556832\n",
            " -0.64629018 -0.46932267 -0.42954562 -0.24658816], B intercept: 0.4459694575892897, Train loss: 0.047960693579683175, Test loss: 0.048892195921256994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 30/100 [00:00<00:00, 120.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Epoch no(iteration no)  29 \n",
            " Train data set : \n",
            "W intercept: [ 0.08510794 -0.47157658 -0.52476011 -0.53053178 -0.4829571  -0.41852787\n",
            " -0.65606652 -0.46981772 -0.439743   -0.25354006], B intercept: 0.44294514034428084, Train loss: 0.047551367383526534, Test loss: 0.048409450842189776\n",
            "\n",
            "-- Epoch no(iteration no)  30 \n",
            " Train data set : \n",
            "W intercept: [ 0.08602497 -0.48035444 -0.53541941 -0.53893062 -0.49215556 -0.41876924\n",
            " -0.66711432 -0.47731761 -0.44661977 -0.25414139], B intercept: 0.4426431638029873, Train loss: 0.04696585864816365, Test loss: 0.047677022879368786\n",
            "\n",
            "-- Epoch no(iteration no)  31 \n",
            " Train data set : \n",
            "W intercept: [ 0.08452981 -0.48503198 -0.52692951 -0.53295322 -0.4983298  -0.43024552\n",
            " -0.67873798 -0.48746845 -0.44745558 -0.25772484], B intercept: 0.45219701153543396, Train loss: 0.0465667451904342, Test loss: 0.04725817605893509\n",
            "\n",
            "-- Epoch no(iteration no)  32 \n",
            " Train data set : \n",
            "W intercept: [ 0.08571358 -0.48363418 -0.53190898 -0.53848706 -0.49909362 -0.4350386\n",
            " -0.67000029 -0.49729477 -0.45046262 -0.25324296], B intercept: 0.4702912613620256, Train loss: 0.0462930248378483, Test loss: 0.04705167196253957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 45/100 [00:00<00:00, 130.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Epoch no(iteration no)  33 \n",
            " Train data set : \n",
            "W intercept: [ 0.08311207 -0.49680385 -0.53364154 -0.54262277 -0.50477011 -0.44325478\n",
            " -0.67730509 -0.5053617  -0.45653523 -0.25412404], B intercept: 0.4706317519173113, Train loss: 0.04585395991692022, Test loss: 0.04645083410021473\n",
            "\n",
            "-- Epoch no(iteration no)  34 \n",
            " Train data set : \n",
            "W intercept: [ 0.08852583 -0.50035529 -0.54066702 -0.54953783 -0.50815963 -0.44808177\n",
            " -0.69041339 -0.51650854 -0.46029376 -0.25570489], B intercept: 0.4730645036547063, Train loss: 0.045376799741770515, Test loss: 0.04587503953449794\n",
            "\n",
            "-- Epoch no(iteration no)  35 \n",
            " Train data set : \n",
            "W intercept: [ 0.0892785  -0.50544031 -0.54691452 -0.55558886 -0.51356525 -0.45393487\n",
            " -0.69689753 -0.52138174 -0.46596223 -0.25831012], B intercept: 0.4803392626016237, Train loss: 0.04495868831535082, Test loss: 0.04538821890755262\n",
            "\n",
            "-- Epoch no(iteration no)  36 \n",
            " Train data set : \n",
            "W intercept: [ 0.09210565 -0.51661108 -0.55273923 -0.56493968 -0.51403413 -0.45798955\n",
            " -0.70286365 -0.53056864 -0.47749291 -0.26579165], B intercept: 0.4839074470832128, Train loss: 0.04448377557836257, Test loss: 0.04471893072316804\n",
            "\n",
            "-- Epoch no(iteration no)  37 \n",
            " Train data set : \n",
            "W intercept: [ 0.09806432 -0.51815093 -0.55865292 -0.57184072 -0.5226217  -0.46196058\n",
            " -0.71735259 -0.53797587 -0.48285881 -0.2665524 ], B intercept: 0.48879557820053215, Train loss: 0.044041800252939536, Test loss: 0.04423812936673029\n",
            "\n",
            "-- Epoch no(iteration no)  38 \n",
            " Train data set : \n",
            "W intercept: [ 0.09582757 -0.51886304 -0.56459915 -0.57908404 -0.52876145 -0.46866729\n",
            " -0.72344874 -0.54433916 -0.48871834 -0.26871542], B intercept: 0.49788290498707743, Train loss: 0.0436814427241855, Test loss: 0.04384379512184476\n",
            "\n",
            "-- Epoch no(iteration no)  39 \n",
            " Train data set : \n",
            "W intercept: [ 0.09609189 -0.53285276 -0.54888927 -0.57112559 -0.52383216 -0.4619202\n",
            " -0.72230099 -0.53615878 -0.48790068 -0.27779328], B intercept: 0.510812078902942, Train loss: 0.04365387491627608, Test loss: 0.04375505596021594\n",
            "\n",
            "-- Epoch no(iteration no)  40 \n",
            " Train data set : \n",
            "W intercept: [ 0.09236529 -0.54164598 -0.5527246  -0.57463197 -0.52400038 -0.4537203\n",
            " -0.73138831 -0.52735102 -0.49126411 -0.28206739], B intercept: 0.5191853543079116, Train loss: 0.04341804133523157, Test loss: 0.04344256836571777\n",
            "\n",
            "-- Epoch no(iteration no)  41 \n",
            " Train data set : \n",
            "W intercept: [ 0.09280814 -0.54831659 -0.55606458 -0.57987906 -0.52162795 -0.44423343\n",
            " -0.72110841 -0.53789664 -0.5012525  -0.28716353], B intercept: 0.5287165684277062, Train loss: 0.043243304018183895, Test loss: 0.04315489671589161\n",
            "\n",
            "-- Epoch no(iteration no)  42 \n",
            " Train data set : \n",
            "W intercept: [ 0.09429609 -0.54895672 -0.56303009 -0.58460451 -0.5334317  -0.44853159\n",
            " -0.72624867 -0.54310286 -0.50593749 -0.28765819], B intercept: 0.5316717368078984, Train loss: 0.04297114700220008, Test loss: 0.04285856096206119\n",
            "\n",
            "-- Epoch no(iteration no)  43 \n",
            " Train data set : \n",
            "W intercept: [ 0.08774922 -0.55076757 -0.56812014 -0.58623306 -0.53651717 -0.44987493\n",
            " -0.72832867 -0.54520918 -0.49905556 -0.29151337], B intercept: 0.5441051957077404, Train loss: 0.04278755891672177, Test loss: 0.04264818817983697\n",
            "\n",
            "-- Epoch no(iteration no)  44 \n",
            " Train data set : \n",
            "W intercept: [ 0.09400778 -0.54248034 -0.57063596 -0.59056818 -0.53184376 -0.45052614\n",
            " -0.72810468 -0.53559657 -0.48775593 -0.29029947], B intercept: 0.5561860314849625, Train loss: 0.04283141642864728, Test loss: 0.04283763155345545\n",
            "\n",
            "-- Epoch no(iteration no)  45 \n",
            " Train data set : \n",
            "W intercept: [ 0.09242651 -0.54827549 -0.57028708 -0.58888534 -0.53691935 -0.43496548\n",
            " -0.74423824 -0.53923213 -0.47631881 -0.28797292], B intercept: 0.5483742787460655, Train loss: 0.04276526521727099, Test loss: 0.042691294167509504\n",
            "\n",
            "-- Epoch no(iteration no)  46 \n",
            " Train data set : \n",
            "W intercept: [ 0.09169731 -0.55654596 -0.57850989 -0.59528205 -0.53799519 -0.43862059\n",
            " -0.74992243 -0.54224105 -0.47983478 -0.29290625], B intercept: 0.5524658327488374, Train loss: 0.04248075907260811, Test loss: 0.04227393634680392\n",
            "\n",
            "-- Epoch no(iteration no)  47 \n",
            " Train data set : \n",
            "W intercept: [ 0.09196738 -0.55997857 -0.58315757 -0.60045244 -0.54181771 -0.44765168\n",
            " -0.76034267 -0.54374115 -0.4832517  -0.2948313 ], B intercept: 0.5574109005243123, Train loss: 0.042206069631320904, Test loss: 0.04194659506230517\n",
            "\n",
            "-- Epoch no(iteration no)  48 \n",
            " Train data set : \n",
            "W intercept: [ 0.09243954 -0.57143227 -0.58656616 -0.60660055 -0.54927834 -0.44996512\n",
            " -0.76836538 -0.54599637 -0.49115327 -0.29692942], B intercept: 0.556368657354883, Train loss: 0.041933818701198784, Test loss: 0.0415385586624685\n",
            "\n",
            "-- Epoch no(iteration no)  49 \n",
            " Train data set : \n",
            "W intercept: [ 0.09840813 -0.56878465 -0.58897572 -0.60462216 -0.54762458 -0.45358563\n",
            " -0.77526621 -0.54622948 -0.49495207 -0.30717336], B intercept: 0.561355644374818, Train loss: 0.04179346138881679, Test loss: 0.04138192272319525\n",
            "\n",
            "-- Epoch no(iteration no)  50 \n",
            " Train data set : \n",
            "W intercept: [ 0.09901142 -0.5672092  -0.59513637 -0.61119668 -0.5577774  -0.44898095\n",
            " -0.78600162 -0.54772783 -0.50545694 -0.30498543], B intercept: 0.5544288037289544, Train loss: 0.041637891483825136, Test loss: 0.041199216685480926\n",
            "\n",
            "-- Epoch no(iteration no)  51 \n",
            " Train data set : \n",
            "W intercept: [ 0.09838784 -0.57527288 -0.60031599 -0.61681983 -0.55937991 -0.45597065\n",
            " -0.78953719 -0.55147435 -0.51681659 -0.30521069], B intercept: 0.5541510482702461, Train loss: 0.041443792400801906, Test loss: 0.04089762600309431\n",
            "\n",
            "-- Epoch no(iteration no)  52 \n",
            " Train data set : \n",
            "W intercept: [ 0.09797812 -0.56949099 -0.58889089 -0.60729264 -0.55958875 -0.46434772\n",
            " -0.78249413 -0.56037734 -0.5008012  -0.30059232], B intercept: 0.574215986951939, Train loss: 0.04148525493415242, Test loss: 0.0410941785153037\n",
            "\n",
            "-- Epoch no(iteration no)  53 \n",
            " Train data set : \n",
            "W intercept: [ 0.09647195 -0.57590752 -0.59104151 -0.61052074 -0.56157821 -0.47131754\n",
            " -0.78749317 -0.57190288 -0.50418282 -0.30201444], B intercept: 0.5757805780745904, Train loss: 0.041288803099685065, Test loss: 0.04078374879023606\n",
            "\n",
            "-- Epoch no(iteration no)  54 \n",
            " Train data set : \n",
            "W intercept: [ 0.09625755 -0.58282157 -0.59376877 -0.61308921 -0.56770546 -0.47315473\n",
            " -0.79526078 -0.57425845 -0.51182822 -0.30308311], B intercept: 0.5773253377896418, Train loss: 0.04109072682654984, Test loss: 0.0405080714256437\n",
            "\n",
            "-- Epoch no(iteration no)  55 \n",
            " Train data set : \n",
            "W intercept: [ 0.10130194 -0.58410935 -0.59842271 -0.61849568 -0.5707326  -0.47697375\n",
            " -0.80363678 -0.57992864 -0.51368003 -0.30383705], B intercept: 0.5793929909746742, Train loss: 0.040916304297496014, Test loss: 0.04029031406858543\n",
            "\n",
            "-- Epoch no(iteration no)  56 \n",
            " Train data set : \n",
            "W intercept: [ 0.10476669 -0.58918906 -0.60050716 -0.62164887 -0.57237498 -0.48095244\n",
            " -0.80972312 -0.58737573 -0.5239643  -0.30404934], B intercept: 0.582133197068952, Train loss: 0.04073269302826548, Test loss: 0.04003673729471061\n",
            "\n",
            "-- Epoch no(iteration no)  57 \n",
            " Train data set : \n",
            "W intercept: [ 0.10474489 -0.59644944 -0.60658976 -0.62960038 -0.5762778  -0.48265368\n",
            " -0.81580555 -0.59093934 -0.52819017 -0.31091387], B intercept: 0.585359611094438, Train loss: 0.040517748238628346, Test loss: 0.039691866512120644\n",
            "\n",
            "-- Epoch no(iteration no)  58 \n",
            " Train data set : \n",
            "W intercept: [ 0.10589036 -0.59780444 -0.61173837 -0.63711783 -0.58022645 -0.486593\n",
            " -0.8235372  -0.59785503 -0.53021771 -0.31153277], B intercept: 0.5897172469793316, Train loss: 0.04033908251941942, Test loss: 0.03946036265968622\n",
            "\n",
            "-- Epoch no(iteration no)  59 \n",
            " Train data set : \n",
            "W intercept: [ 0.11004134 -0.60583545 -0.59334281 -0.62616402 -0.57312923 -0.4794914\n",
            " -0.82156334 -0.59182926 -0.52985133 -0.31937897], B intercept: 0.5999605236223424, Train loss: 0.04035084502640601, Test loss: 0.03947486635763322\n",
            "\n",
            "-- Epoch no(iteration no)  60 \n",
            " Train data set : \n",
            "W intercept: [ 0.11133013 -0.61140593 -0.59858621 -0.63141992 -0.57614114 -0.48146191\n",
            " -0.82881058 -0.59251812 -0.53345205 -0.32131185], B intercept: 0.6043029269656548, Train loss: 0.040180448781573556, Test loss: 0.03923625906576964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████    | 61/100 [00:00<00:00, 140.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Epoch no(iteration no)  61 \n",
            " Train data set : \n",
            "W intercept: [ 0.10366553 -0.62025122 -0.59804122 -0.63244424 -0.56938605 -0.45981183\n",
            " -0.8155083  -0.58880235 -0.54078465 -0.3280166 ], B intercept: 0.6172927919905807, Train loss: 0.04015266841469453, Test loss: 0.039129543189982983\n",
            "\n",
            "-- Epoch no(iteration no)  62 \n",
            " Train data set : \n",
            "W intercept: [ 0.10463888 -0.62046708 -0.60405791 -0.63682434 -0.57898084 -0.46272387\n",
            " -0.8189293  -0.59214071 -0.54499841 -0.32820244], B intercept: 0.6194347489714024, Train loss: 0.040026876325864394, Test loss: 0.03898054361819885\n",
            "\n",
            "-- Epoch no(iteration no)  63 \n",
            " Train data set : \n",
            "W intercept: [ 0.09814679 -0.62046076 -0.60766814 -0.63702015 -0.58150005 -0.46306282\n",
            " -0.81990411 -0.59406789 -0.53694537 -0.33254019], B intercept: 0.6311885390140783, Train loss: 0.03993364491895099, Test loss: 0.03887037312122397\n",
            "\n",
            "-- Epoch no(iteration no)  64 \n",
            " Train data set : \n",
            "W intercept: [ 0.10252715 -0.61036629 -0.60850041 -0.63784462 -0.57632594 -0.4621965\n",
            " -0.81439021 -0.58246119 -0.52353764 -0.33117035], B intercept: 0.6450926184481256, Train loss: 0.040024476826093114, Test loss: 0.03913622081406706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 76/100 [00:00<00:00, 140.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Epoch no(iteration no)  65 \n",
            " Train data set : \n",
            "W intercept: [ 0.10369779 -0.61401776 -0.61229846 -0.64313857 -0.5784006  -0.46725706\n",
            " -0.8229063  -0.58647645 -0.52820018 -0.33143148], B intercept: 0.6437959668000363, Train loss: 0.039890625652020444, Test loss: 0.03892882418894481\n",
            "\n",
            "-- Epoch no(iteration no)  66 \n",
            " Train data set : \n",
            "W intercept: [ 0.1020967  -0.61903948 -0.61116796 -0.64055353 -0.5827148  -0.44885384\n",
            " -0.83615658 -0.58785894 -0.51441084 -0.32907503], B intercept: 0.6369897387954947, Train loss: 0.039861949146269254, Test loss: 0.03886102796941541\n",
            "\n",
            "-- Epoch no(iteration no)  67 \n",
            " Train data set : \n",
            "W intercept: [ 0.10179846 -0.6261315  -0.61722228 -0.64553398 -0.584087   -0.45652818\n",
            " -0.84550439 -0.58751361 -0.51604617 -0.33262618], B intercept: 0.6380324117830265, Train loss: 0.039706537593667796, Test loss: 0.038603801553554175\n",
            "\n",
            "-- Epoch no(iteration no)  68 \n",
            " Train data set : \n",
            "W intercept: [ 0.10175042 -0.63390878 -0.61957919 -0.64975169 -0.58925585 -0.45761812\n",
            " -0.85516166 -0.58968005 -0.52305153 -0.3337728 ], B intercept: 0.6359752571725614, Train loss: 0.03955915004702877, Test loss: 0.038358067364862136\n",
            "\n",
            "-- Epoch no(iteration no)  69 \n",
            " Train data set : \n",
            "W intercept: [ 0.10526048 -0.63036494 -0.6214215  -0.64796154 -0.5919306  -0.46118822\n",
            " -0.85987614 -0.59046807 -0.52601917 -0.33789232], B intercept: 0.6469031246415383, Train loss: 0.039451372406090465, Test loss: 0.03828496290452355\n",
            "\n",
            "-- Epoch no(iteration no)  70 \n",
            " Train data set : \n",
            "W intercept: [ 0.10813449 -0.62744692 -0.6246686  -0.6522122  -0.59386466 -0.46041892\n",
            " -0.86576836 -0.58800309 -0.53387171 -0.34331468], B intercept: 0.6380810359093818, Train loss: 0.039430299373626965, Test loss: 0.03823038451975313\n",
            "\n",
            "-- Epoch no(iteration no)  71 \n",
            " Train data set : \n",
            "W intercept: [ 0.10790826 -0.63513628 -0.63016402 -0.65765562 -0.59634712 -0.46235728\n",
            " -0.87008656 -0.59143243 -0.54613975 -0.34059261], B intercept: 0.6294402787910218, Train loss: 0.03935963112649538, Test loss: 0.038052132802733826\n",
            "\n",
            "-- Epoch no(iteration no)  72 \n",
            " Train data set : \n",
            "W intercept: [ 0.10663234 -0.63418284 -0.61846998 -0.64687809 -0.60270937 -0.46845093\n",
            " -0.87876238 -0.59851437 -0.53584404 -0.34431501], B intercept: 0.6397962555874857, Train loss: 0.03927806442345953, Test loss: 0.038007397070531114\n",
            "\n",
            "-- Epoch no(iteration no)  73 \n",
            " Train data set : \n",
            "W intercept: [ 0.10669673 -0.63044543 -0.6209079  -0.65001636 -0.59997462 -0.47121794\n",
            " -0.86607544 -0.60503829 -0.53479831 -0.33800063], B intercept: 0.6544128084649674, Train loss: 0.03927933910652711, Test loss: 0.0380727908774694\n",
            "\n",
            "-- Epoch no(iteration no)  74 \n",
            " Train data set : \n",
            "W intercept: [ 0.10489609 -0.64057159 -0.62096492 -0.65124856 -0.60288427 -0.47704182\n",
            " -0.87042679 -0.61161987 -0.53870554 -0.33716617], B intercept: 0.6510668493855447, Train loss: 0.0391932460731071, Test loss: 0.037873062904846114\n",
            "\n",
            "-- Epoch no(iteration no)  75 \n",
            " Train data set : \n",
            "W intercept: [ 0.1085998  -0.64198985 -0.62446962 -0.6555689  -0.60443393 -0.47912016\n",
            " -0.87862315 -0.61831524 -0.53966838 -0.33747154], B intercept: 0.6506845943379017, Train loss: 0.03909826425458432, Test loss: 0.03772659454659717\n",
            "\n",
            "-- Epoch no(iteration no)  76 \n",
            " Train data set : \n",
            "W intercept: [ 0.10903636 -0.64504259 -0.62783434 -0.65906294 -0.60755464 -0.48234862\n",
            " -0.88270011 -0.62155673 -0.54294484 -0.33824245], B intercept: 0.6553150204420728, Train loss: 0.038999133351521005, Test loss: 0.037583737264235785\n",
            "\n",
            "-- Epoch no(iteration no)  77 \n",
            " Train data set : \n",
            "W intercept: [ 0.11221485 -0.65188563 -0.63116723 -0.66535116 -0.60743502 -0.48475535\n",
            " -0.88653691 -0.62689943 -0.55173407 -0.34428251], B intercept: 0.6545578461341502, Train loss: 0.03888946050564358, Test loss: 0.037351402786113576\n",
            "\n",
            "-- Epoch no(iteration no)  78 \n",
            " Train data set : \n",
            "W intercept: [ 0.11606578 -0.65260254 -0.63554329 -0.67055709 -0.61173702 -0.48705526\n",
            " -0.89521842 -0.63302723 -0.55385262 -0.34441818], B intercept: 0.6569610906805504, Train loss: 0.03878183223418629, Test loss: 0.037205982233832854\n",
            "\n",
            "-- Epoch no(iteration no)  79 \n",
            " Train data set : \n",
            "W intercept: [ 0.11492656 -0.65252931 -0.63833713 -0.67440095 -0.61551428 -0.49134292\n",
            " -0.89940074 -0.63580091 -0.55792805 -0.34601256], B intercept: 0.6635703600927395, Train loss: 0.038693311319256324, Test loss: 0.037096061022478095\n",
            "\n",
            "-- Epoch no(iteration no)  80 \n",
            " Train data set : \n",
            "W intercept: [ 0.11616274 -0.66406548 -0.62011876 -0.66350514 -0.60778103 -0.48261651\n",
            " -0.89573315 -0.62676297 -0.55618819 -0.35361175], B intercept: 0.6728171626052121, Train loss: 0.03867956977300328, Test loss: 0.03707202889637522\n",
            "\n",
            "-- Epoch no(iteration no)  81 \n",
            " Train data set : \n",
            "W intercept: [ 0.11232721 -0.6705867  -0.62142895 -0.66466762 -0.60591807 -0.47280824\n",
            " -0.90084244 -0.61593548 -0.55794609 -0.35707276], B intercept: 0.6803088310741693, Train loss: 0.03859985645627005, Test loss: 0.03696599628486838\n",
            "\n",
            "-- Epoch no(iteration no)  82 \n",
            " Train data set : \n",
            "W intercept: [ 0.11225163 -0.67567103 -0.62268151 -0.66769169 -0.60182741 -0.4616614\n",
            " -0.88762014 -0.62429478 -0.56574988 -0.36103277], B intercept: 0.6878633406784592, Train loss: 0.03856519280693977, Test loss: 0.036856449504649356\n",
            "\n",
            "-- Epoch no(iteration no)  83 \n",
            " Train data set : \n",
            "W intercept: [ 0.1130028  -0.67542379 -0.62779817 -0.67075328 -0.61145538 -0.46418127\n",
            " -0.89057049 -0.62733667 -0.56893275 -0.36099507], B intercept: 0.688758457684048, Train loss: 0.03850138321009885, Test loss: 0.036771563607327064\n",
            "\n",
            "-- Epoch no(iteration no)  84 \n",
            " Train data set : \n",
            "W intercept: [ 0.10617731 -0.67577524 -0.63096856 -0.6707348  -0.61301592 -0.46382534\n",
            " -0.89058099 -0.62769496 -0.56050311 -0.36437199], B intercept: 0.6993539410856003, Train loss: 0.03844754826181661, Test loss: 0.036704307916365864\n",
            "\n",
            "-- Epoch no(iteration no)  85 \n",
            " Train data set : \n",
            "W intercept: [ 0.11144538 -0.66548152 -0.63163085 -0.67270604 -0.60693625 -0.46285898\n",
            " -0.88758192 -0.61670934 -0.5468212  -0.36231102], B intercept: 0.7097955673907467, Train loss: 0.038509342025968146, Test loss: 0.03692731073374428\n",
            "\n",
            "-- Epoch no(iteration no)  86 \n",
            " Train data set : \n",
            "W intercept: [ 0.10999209 -0.66997522 -0.62929353 -0.66919545 -0.61051545 -0.44628667\n",
            " -0.90097509 -0.61792538 -0.53422491 -0.35929546], B intercept: 0.7004746322476151, Train loss: 0.03848794219546522, Test loss: 0.03688530799978167\n",
            "\n",
            "-- Epoch no(iteration no)  87 \n",
            " Train data set : \n",
            "W intercept: [ 0.10933401 -0.67628913 -0.63496624 -0.6732455  -0.61058327 -0.44838784\n",
            " -0.90449133 -0.61948856 -0.53612497 -0.36293429], B intercept: 0.7021402839952542, Train loss: 0.03840467653903181, Test loss: 0.0367087725494802\n",
            "\n",
            "-- Epoch no(iteration no)  88 \n",
            " Train data set : \n",
            "W intercept: [ 0.10927073 -0.67835304 -0.637889   -0.67655517 -0.6130417  -0.45540314\n",
            " -0.91265216 -0.61989354 -0.53781136 -0.3638838 ], B intercept: 0.7046340582050908, Train loss: 0.03832443299619289, Test loss: 0.03659063499308712\n",
            "\n",
            "-- Epoch no(iteration no)  89 \n",
            " Train data set : \n",
            "W intercept: [ 0.10947703 -0.68672405 -0.63983141 -0.68074521 -0.61859966 -0.45614216\n",
            " -0.91871755 -0.62112314 -0.5444806  -0.36490002], B intercept: 0.7025170142533693, Train loss: 0.03823665814649189, Test loss: 0.036410446934659065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|█████████ | 91/100 [00:00<00:00, 140.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Epoch no(iteration no)  90 \n",
            " Train data set : \n",
            "W intercept: [ 0.11554366 -0.68244514 -0.64061745 -0.67725901 -0.61559831 -0.4585645\n",
            " -0.92424948 -0.62020462 -0.54690155 -0.37455017], B intercept: 0.7050411420879673, Train loss: 0.038200686405361964, Test loss: 0.03637960546468682\n",
            "\n",
            "-- Epoch no(iteration no)  91 \n",
            " Train data set : \n",
            "W intercept: [ 0.11522569 -0.67966075 -0.64470871 -0.68234967 -0.62369043 -0.45291492\n",
            " -0.93220157 -0.62034382 -0.55646786 -0.37124124], B intercept: 0.6951785100388982, Train loss: 0.0381830594843624, Test loss: 0.036353098890842706\n",
            "\n",
            "-- Epoch no(iteration no)  92 \n",
            " Train data set : \n",
            "W intercept: [ 0.11441617 -0.68580983 -0.64796414 -0.68626185 -0.62358289 -0.45892553\n",
            " -0.93405793 -0.62203173 -0.56669999 -0.37059633], B intercept: 0.6927700222044486, Train loss: 0.03814338241515253, Test loss: 0.03623386598200481\n",
            "\n",
            "-- Epoch no(iteration no)  93 \n",
            " Train data set : \n",
            "W intercept: [ 0.11340286 -0.67838652 -0.63519376 -0.67517382 -0.62272819 -0.4661787\n",
            " -0.92469982 -0.62978101 -0.54930744 -0.36542593], B intercept: 0.7118874710837079, Train loss: 0.03818102019234461, Test loss: 0.03642159129190205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 100/100 [00:00<00:00, 135.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Epoch no(iteration no)  94 \n",
            " Train data set : \n",
            "W intercept: [ 0.11123186 -0.68546249 -0.63572744 -0.67673535 -0.62560865 -0.46899253\n",
            " -0.92850466 -0.63835055 -0.5531401  -0.36364335], B intercept: 0.7051152980628579, Train loss: 0.0381438612269627, Test loss: 0.036282811948670515\n",
            "\n",
            "-- Epoch no(iteration no)  95 \n",
            " Train data set : \n",
            "W intercept: [ 0.11146031 -0.68936293 -0.63773924 -0.67864582 -0.62822654 -0.47275384\n",
            " -0.93352325 -0.64073762 -0.55704343 -0.36602432], B intercept: 0.7108848752259179, Train loss: 0.03805813017006961, Test loss: 0.03614734402205374\n",
            "\n",
            "-- Epoch no(iteration no)  96 \n",
            " Train data set : \n",
            "W intercept: [ 0.11560086 -0.69012085 -0.64085952 -0.68253157 -0.62996258 -0.47524548\n",
            " -0.9399751  -0.64507299 -0.55791248 -0.36630653], B intercept: 0.7116440264288814, Train loss: 0.038001272813846174, Test loss: 0.0360587771616527\n",
            "\n",
            "-- Epoch no(iteration no)  97 \n",
            " Train data set : \n",
            "W intercept: [ 0.11883167 -0.69381414 -0.64166598 -0.68449877 -0.63058948 -0.47814552\n",
            " -0.94435804 -0.65058296 -0.56653994 -0.36608037], B intercept: 0.7126109971037337, Train loss: 0.03794301422043845, Test loss: 0.03595006795924687\n",
            "\n",
            "-- Epoch no(iteration no)  98 \n",
            " Train data set : \n",
            "W intercept: [ 0.1187904  -0.69898527 -0.64633147 -0.6910314  -0.63318223 -0.47922211\n",
            " -0.9487588  -0.65327101 -0.56969385 -0.37176561], B intercept: 0.7143503644841149, Train loss: 0.037863175054264, Test loss: 0.0357730730221217\n",
            "\n",
            "-- Epoch no(iteration no)  99 \n",
            " Train data set : \n",
            "W intercept: [ 0.12032839 -0.69981131 -0.65009668 -0.69660232 -0.63585051 -0.48196324\n",
            " -0.95469534 -0.6587617  -0.5705219  -0.37200612], B intercept: 0.716961047838447, Train loss: 0.03780286494503049, Test loss: 0.03567113488274088\n",
            "\n",
            "-- Epoch no(iteration no)  100 \n",
            " Train data set : \n",
            "W intercept: [ 0.1240405  -0.70731143 -0.63080351 -0.68440907 -0.62764631 -0.47376225\n",
            " -0.9512984  -0.65181964 -0.56979707 -0.37929183], B intercept: 0.7257831274737563, Train loss: 0.03778104242129138, Test loss: 0.03568229635622914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
        "clf\n",
        "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
        "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
        "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
        "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
        "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
        "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)\n",
        "clf.fit(X=X_train, y=y_train)"
      ],
      "metadata": {
        "id": "1JbW9eh7KVu4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8c54080-46a8-438c-ac8f-c7615d7c83a5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 0.05, NNZs: 10, Bias: 0.008440, T: 512, Avg. loss: 0.665637\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.10, NNZs: 10, Bias: 0.016754, T: 1024, Avg. loss: 0.615597\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.14, NNZs: 10, Bias: 0.024958, T: 1536, Avg. loss: 0.572475\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.19, NNZs: 10, Bias: 0.033035, T: 2048, Avg. loss: 0.535126\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.22, NNZs: 10, Bias: 0.040970, T: 2560, Avg. loss: 0.502597\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.26, NNZs: 10, Bias: 0.048754, T: 3072, Avg. loss: 0.474082\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.29, NNZs: 10, Bias: 0.056349, T: 3584, Avg. loss: 0.448904\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.33, NNZs: 10, Bias: 0.063790, T: 4096, Avg. loss: 0.426542\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.36, NNZs: 10, Bias: 0.071066, T: 4608, Avg. loss: 0.406571\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.39, NNZs: 10, Bias: 0.078188, T: 5120, Avg. loss: 0.388645\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.41, NNZs: 10, Bias: 0.085137, T: 5632, Avg. loss: 0.372468\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.44, NNZs: 10, Bias: 0.091920, T: 6144, Avg. loss: 0.357795\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.46, NNZs: 10, Bias: 0.098543, T: 6656, Avg. loss: 0.344435\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 0.49, NNZs: 10, Bias: 0.105008, T: 7168, Avg. loss: 0.332215\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 0.51, NNZs: 10, Bias: 0.111327, T: 7680, Avg. loss: 0.321004\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 0.53, NNZs: 10, Bias: 0.117487, T: 8192, Avg. loss: 0.310682\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 0.55, NNZs: 10, Bias: 0.123503, T: 8704, Avg. loss: 0.301149\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 0.57, NNZs: 10, Bias: 0.129384, T: 9216, Avg. loss: 0.292323\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 0.59, NNZs: 10, Bias: 0.135133, T: 9728, Avg. loss: 0.284126\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 0.61, NNZs: 10, Bias: 0.140757, T: 10240, Avg. loss: 0.276496\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 0.63, NNZs: 10, Bias: 0.146256, T: 10752, Avg. loss: 0.269373\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 0.64, NNZs: 10, Bias: 0.151632, T: 11264, Avg. loss: 0.262713\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 0.66, NNZs: 10, Bias: 0.156895, T: 11776, Avg. loss: 0.256472\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 0.68, NNZs: 10, Bias: 0.162047, T: 12288, Avg. loss: 0.250615\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 0.69, NNZs: 10, Bias: 0.167092, T: 12800, Avg. loss: 0.245102\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 0.71, NNZs: 10, Bias: 0.172029, T: 13312, Avg. loss: 0.239910\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 0.72, NNZs: 10, Bias: 0.176867, T: 13824, Avg. loss: 0.235009\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 0.74, NNZs: 10, Bias: 0.181613, T: 14336, Avg. loss: 0.230376\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 0.75, NNZs: 10, Bias: 0.186263, T: 14848, Avg. loss: 0.225990\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 0.77, NNZs: 10, Bias: 0.190825, T: 15360, Avg. loss: 0.221833\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 0.78, NNZs: 10, Bias: 0.195299, T: 15872, Avg. loss: 0.217886\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 0.79, NNZs: 10, Bias: 0.199687, T: 16384, Avg. loss: 0.214136\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 0.80, NNZs: 10, Bias: 0.203999, T: 16896, Avg. loss: 0.210569\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 0.82, NNZs: 10, Bias: 0.208230, T: 17408, Avg. loss: 0.207170\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 0.83, NNZs: 10, Bias: 0.212386, T: 17920, Avg. loss: 0.203931\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 0.84, NNZs: 10, Bias: 0.216468, T: 18432, Avg. loss: 0.200838\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 0.85, NNZs: 10, Bias: 0.220483, T: 18944, Avg. loss: 0.197884\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 0.86, NNZs: 10, Bias: 0.224428, T: 19456, Avg. loss: 0.195060\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 0.87, NNZs: 10, Bias: 0.228307, T: 19968, Avg. loss: 0.192356\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 0.89, NNZs: 10, Bias: 0.232123, T: 20480, Avg. loss: 0.189765\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 0.90, NNZs: 10, Bias: 0.235877, T: 20992, Avg. loss: 0.187280\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 0.91, NNZs: 10, Bias: 0.239570, T: 21504, Avg. loss: 0.184896\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 0.92, NNZs: 10, Bias: 0.243203, T: 22016, Avg. loss: 0.182608\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 0.93, NNZs: 10, Bias: 0.246780, T: 22528, Avg. loss: 0.180408\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 0.94, NNZs: 10, Bias: 0.250304, T: 23040, Avg. loss: 0.178292\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 0.94, NNZs: 10, Bias: 0.253775, T: 23552, Avg. loss: 0.176256\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 0.95, NNZs: 10, Bias: 0.257194, T: 24064, Avg. loss: 0.174296\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 0.96, NNZs: 10, Bias: 0.260562, T: 24576, Avg. loss: 0.172407\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 0.97, NNZs: 10, Bias: 0.263882, T: 25088, Avg. loss: 0.170586\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 0.98, NNZs: 10, Bias: 0.267155, T: 25600, Avg. loss: 0.168829\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 0.99, NNZs: 10, Bias: 0.270382, T: 26112, Avg. loss: 0.167133\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 1.00, NNZs: 10, Bias: 0.273565, T: 26624, Avg. loss: 0.165494\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 1.01, NNZs: 10, Bias: 0.276705, T: 27136, Avg. loss: 0.163910\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 1.01, NNZs: 10, Bias: 0.279802, T: 27648, Avg. loss: 0.162379\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 1.02, NNZs: 10, Bias: 0.282857, T: 28160, Avg. loss: 0.160898\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 1.03, NNZs: 10, Bias: 0.285873, T: 28672, Avg. loss: 0.159465\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 1.04, NNZs: 10, Bias: 0.288848, T: 29184, Avg. loss: 0.158076\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 1.05, NNZs: 10, Bias: 0.291787, T: 29696, Avg. loss: 0.156731\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 1.05, NNZs: 10, Bias: 0.294687, T: 30208, Avg. loss: 0.155427\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 1.06, NNZs: 10, Bias: 0.297552, T: 30720, Avg. loss: 0.154163\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 1.07, NNZs: 10, Bias: 0.300382, T: 31232, Avg. loss: 0.152937\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 1.08, NNZs: 10, Bias: 0.303176, T: 31744, Avg. loss: 0.151747\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 1.08, NNZs: 10, Bias: 0.305939, T: 32256, Avg. loss: 0.150592\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 1.09, NNZs: 10, Bias: 0.308667, T: 32768, Avg. loss: 0.149470\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 1.10, NNZs: 10, Bias: 0.311364, T: 33280, Avg. loss: 0.148380\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 1.10, NNZs: 10, Bias: 0.314030, T: 33792, Avg. loss: 0.147320\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 1.11, NNZs: 10, Bias: 0.316665, T: 34304, Avg. loss: 0.146289\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 1.12, NNZs: 10, Bias: 0.319270, T: 34816, Avg. loss: 0.145287\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 1.12, NNZs: 10, Bias: 0.321845, T: 35328, Avg. loss: 0.144312\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 1.13, NNZs: 10, Bias: 0.324393, T: 35840, Avg. loss: 0.143362\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 1.14, NNZs: 10, Bias: 0.326913, T: 36352, Avg. loss: 0.142438\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 1.14, NNZs: 10, Bias: 0.329406, T: 36864, Avg. loss: 0.141538\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 1.15, NNZs: 10, Bias: 0.331871, T: 37376, Avg. loss: 0.140662\n",
            "Total training time: 0.05 seconds.\n",
            "Convergence after 73 epochs took 0.05 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
              "              random_state=15, verbose=2)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w-clf.coef_, b-clf.intercept_"
      ],
      "metadata": {
        "id": "W9Ace8j3OF-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76319449-f791-47ee-a451-86c1bf377d69"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 0.06893549, -0.32363451, -0.21685541, -0.26181954, -0.25147749,\n",
              "         -0.13675792, -0.46621668, -0.26440017, -0.20105961, -0.17291884]]),\n",
              " array([0.39391256]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict Function"
      ],
      "metadata": {
        "id": "WRvSQDV8eVRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pred(w,b, X):\n",
        "    N = len(X)\n",
        "    predict = []\n",
        "    for i in range(N):\n",
        "        z=np.dot(w,X[i])+b\n",
        "        # Idk why the confidence interval is messed up \n",
        "        if sigmoid(z) >= 0.5: \n",
        "            predict.append(1)\n",
        "        else:\n",
        "            predict.append(0)\n",
        "    return np.array(predict)\n",
        "\n",
        "print(1-np.sum(y_train - pred(w,b,X_train))/len(X_train))\n",
        "print(1-np.sum(y_test  - pred(w,b,X_test))/len(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w3wDqyjOn6R",
        "outputId": "16daff03-d36c-4ffc-c81f-237818dcb0e2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.994140625\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wgrBIrVEQ7Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = np.arange(1, epochs+1, 1)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, train_loss, label='Train Loss')\n",
        "plt.plot(epochs, test_loss, label='Test Loss')\n",
        "plt.title('Epoch vs Train,Test Loss')\n",
        "plt.xlabel(\"Epoch_no\")\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "print(100*'==')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "OG9PuXvoQ82b",
        "outputId": "7c067fe7-37a2-4253-95cf-f3a49c343b00"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================================================================================================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}