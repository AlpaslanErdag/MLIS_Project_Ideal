{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_classifier.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNurRs0Vil8a7DKIa2X3mdS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anshuman-37/MLIS_Project_Ideal/blob/main/Logistic_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading data"
      ],
      "metadata": {
        "id": "Uww_zAIyC6F1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rPgBnpUuCYxo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/tumor.csv')"
      ],
      "metadata": {
        "id": "3_zd1JTRCiRM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haiRAO59C1C0",
        "outputId": "3c4c13aa-7214-4d4a-c6e7-ff30479a61dc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 683 entries, 0 to 682\n",
            "Data columns (total 11 columns):\n",
            " #   Column                       Non-Null Count  Dtype\n",
            "---  ------                       --------------  -----\n",
            " 0   Sample code number           683 non-null    int64\n",
            " 1   Clump Thickness              683 non-null    int64\n",
            " 2   Uniformity of Cell Size      683 non-null    int64\n",
            " 3   Uniformity of Cell Shape     683 non-null    int64\n",
            " 4   Marginal Adhesion            683 non-null    int64\n",
            " 5   Single Epithelial Cell Size  683 non-null    int64\n",
            " 6   Bare Nuclei                  683 non-null    int64\n",
            " 7   Bland Chromatin              683 non-null    int64\n",
            " 8   Normal Nucleoli              683 non-null    int64\n",
            " 9   Mitoses                      683 non-null    int64\n",
            " 10  Class                        683 non-null    int64\n",
            "dtypes: int64(11)\n",
            "memory usage: 58.8 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a classifier"
      ],
      "metadata": {
        "id": "KF62ewDEDA_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting X & Y\n"
      ],
      "metadata": {
        "id": "E_HVCxZiW8E8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO \n",
        "# have to randomize the data first\n",
        "# After that have to divide in X and Y \n",
        "\n",
        "## Target Values \n",
        "X = df.drop(columns=['Class'])\n",
        "X = np.array(X)\n",
        "print(X.shape)\n",
        "\n",
        "## Label Values\n",
        "Y = df['Class']\n",
        "Y = np.array(Y)\n",
        "print(Y.shape)\n",
        "\n",
        "## Storing the number of rows and columns in X\n",
        "rows , cols = X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGU0KqqjW8Xw",
        "outputId": "8d43d160-69ce-47cc-85b9-6dfba5be8ef9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(683, 10)\n",
            "(683,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intialize the weights"
      ],
      "metadata": {
        "id": "MhCmi37GYpRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(dim):\n",
        "    ''' In this function, we will initialize our weights and bias'''\n",
        "    # Here dimenstion refer to the number of the attributes in the data\n",
        "    w = np.zeros(shape=(dim))\n",
        "    b = 0\n",
        "    return w,b"
      ],
      "metadata": {
        "id": "pRqb-07xDD6m"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Just to check wether the function is working fine\n",
        "w,b = initialize_weights(cols)\n",
        "print('w =',(w))\n",
        "print('b =',b)\n",
        "print('w',np.sum(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z96iV0wuDD9Y",
        "outputId": "f33fabff-401f-4b9b-87a5-94b786611375"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "b = 0\n",
            "w 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grader Function - Weights\n",
        "    Check whether things are working fine or not"
      ],
      "metadata": {
        "id": "Yy9l9puJZHHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Grader Function\n",
        "## Dont run this cell untill and unless you want to check wether everything is fine or not \n",
        "w,b = initialize_weights(cols)\n",
        "def grader_weights(w,b):\n",
        "  assert((len(w)==cols) and b==0 and np.sum(w)==0.0)\n",
        "  return True\n",
        "grader_weights(w,b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Nzv0I31DEAT",
        "outputId": "99a608a4-3a57-479b-eed1-47b48eece72f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Sigmoid\n",
        "\n",
        "![Sigmoid Function](https://www.gstatic.com/education/formulas2/397133473/en/sigmoid_function.svg)"
      ],
      "metadata": {
        "id": "O8piuV5rZUtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    ''' In this function, we will return sigmoid of z'''\n",
        "    # compute sigmoid(z) and return\n",
        "    return 1/(1+np.exp(-z))"
      ],
      "metadata": {
        "id": "CWYiNX80ZUCS"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Log Loss \n",
        "![Log Loss](https://miro.medium.com/max/1192/1*wilGXrItaMAJmZNl6RJq9Q.png)"
      ],
      "metadata": {
        "id": "a_7VxHj6bvEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(y_labels,y_predicted):\n",
        "    '''This function will return the log loss of the function'''\n",
        "    loss = -1 * (np.sum((y_labels * np.log10(y_predicted))+ \\\n",
        "                      ((1-y_labels)*np.log10(1-y_predicted))))/len(y_labels)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "UAnfSyDrDECM"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grader Function log_loss"
      ],
      "metadata": {
        "id": "LzAWM5XFejHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grader_logloss(true,pred):\n",
        "    loss=log_loss(t,p)\n",
        "    assert(loss==0.07644900402910389)\n",
        "    return True\n",
        "true=np.array([1,1,0,1,0])\n",
        "pred=np.array([0.9,0.8,0.1,0.8,0.2])\n",
        "grader_logloss(true,pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emvDMysqDEE9",
        "outputId": "d5478949-04e7-4ba1-faa3-692e6de9fd65"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient with respect w (dw)\n",
        "![Differntiation of cost function wrt to w](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-35e8bb42947bd888580c2a8a9fe8fe0e_l3.svg)"
      ],
      "metadata": {
        "id": "9ZR6i_Qef-4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_dw(x,y,w,b,alpha,N):\n",
        "    '''In this function, we will compute the gardient w.r.to w '''\n",
        "    # Calculcating the graindent of weighted vectors\n",
        "    return x * (y - sigmoid(np.dot(w, x) + b)) - alpha/N*w"
      ],
      "metadata": {
        "id": "_haqFGTcgAg-"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grader_dw(x,y,w,b,alpha,N):\n",
        "    grad_dw=gradient_dw(x,y,w,b,alpha,N)\n",
        "    assert(np.sum(grad_dw)==2.7259648199999997)\n",
        "    return True\n",
        "\n",
        "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
        "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725])\n",
        "\n",
        "grad_y=0\n",
        "grad_w,grad_b=initialize_weights(cols)\n",
        "alpha=0.0001\n",
        "N=rows\n",
        "grader_dw(grad_x,grad_y,grad_w,grad_b,alpha,N)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c9X9RsKgBkg",
        "outputId": "a04f6c73-e371-48f4-ebab-4b494fd85358"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute gradient w.r.to 'b'"
      ],
      "metadata": {
        "id": "IVFmsUW3stPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_db(x,y,w,b):\n",
        "    '''In this function, we will compute gradient w.r.to b '''\n",
        "    # Calculating the gradient of bais\n",
        "    return y - sigmoid(np.dot(w, x) + b)\n",
        "Grader function - 5\n",
        "\n",
        "def grader_db(x,y,w,b):\n",
        "  grad_db=gradient_db(x,y,w,b)\n",
        "  assert(grad_db==-0.5)\n",
        "  return True\n",
        "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
        "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
        "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
        "grad_y=0\n",
        "grad_w,grad_b=initialize_weights(grad_x)\n",
        "alpha=0.0001\n",
        "N=len(X_train)\n",
        "grader_db(grad_x,grad_y,grad_w,grad_b)\n",
        "True"
      ],
      "metadata": {
        "id": "AERrDcGGstfy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}