{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "E_HVCxZiW8E8",
        "MhCmi37GYpRx",
        "Yy9l9puJZHHX",
        "O8piuV5rZUtD",
        "a_7VxHj6bvEf",
        "LzAWM5XFejHV",
        "9ZR6i_Qef-4x",
        "IVFmsUW3stPb",
        "CgkNsFilIy0F"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPyG+QtBdNvkDdwkLvsel4Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anshuman-37/MLIS_Project_Ideal/blob/main/Logistic_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading data"
      ],
      "metadata": {
        "id": "Uww_zAIyC6F1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rPgBnpUuCYxo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xlrd\n",
        "!pip install openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4UuMENmceho",
        "outputId": "3a5cd77d-0209-4662-9783-7dab35a559f3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (2.5.9)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.4.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"breast-cancer-wisconsin.names\") as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4hhSfAecg38",
        "outputId": "05e95f59-1426-4895-8da6-4a866cd6ccdc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Citation Request:\n",
            "   This breast cancer databases was obtained from the University of Wisconsin\n",
            "   Hospitals, Madison from Dr. William H. Wolberg.  If you publish results\n",
            "   when using this database, then please include this information in your\n",
            "   acknowledgements.  Also, please cite one or more of:\n",
            "\n",
            "   1. O. L. Mangasarian and W. H. Wolberg: \"Cancer diagnosis via linear \n",
            "      programming\", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.\n",
            "\n",
            "   2. William H. Wolberg and O.L. Mangasarian: \"Multisurface method of \n",
            "      pattern separation for medical diagnosis applied to breast cytology\", \n",
            "      Proceedings of the National Academy of Sciences, U.S.A., Volume 87, \n",
            "      December 1990, pp 9193-9196.\n",
            "\n",
            "   3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg: \"Pattern recognition \n",
            "      via linear programming: Theory and application to medical diagnosis\", \n",
            "      in: \"Large-scale numerical optimization\", Thomas F. Coleman and Yuying\n",
            "      Li, editors, SIAM Publications, Philadelphia 1990, pp 22-30.\n",
            "\n",
            "   4. K. P. Bennett & O. L. Mangasarian: \"Robust linear programming \n",
            "      discrimination of two linearly inseparable sets\", Optimization Methods\n",
            "      and Software 1, 1992, 23-34 (Gordon & Breach Science Publishers).\n",
            "\n",
            "1. Title: Wisconsin Breast Cancer Database (January 8, 1991)\n",
            "\n",
            "2. Sources:\n",
            "   -- Dr. WIlliam H. Wolberg (physician)\n",
            "      University of Wisconsin Hospitals\n",
            "      Madison, Wisconsin\n",
            "      USA\n",
            "   -- Donor: Olvi Mangasarian (mangasarian@cs.wisc.edu)\n",
            "      Received by David W. Aha (aha@cs.jhu.edu)\n",
            "   -- Date: 15 July 1992\n",
            "\n",
            "3. Past Usage:\n",
            "\n",
            "   Attributes 2 through 10 have been used to represent instances.\n",
            "   Each instance has one of 2 possible classes: benign or malignant.\n",
            "\n",
            "   1. Wolberg,~W.~H., \\& Mangasarian,~O.~L. (1990). Multisurface method of \n",
            "      pattern separation for medical diagnosis applied to breast cytology. In\n",
            "      {\\it Proceedings of the National Academy of Sciences}, {\\it 87},\n",
            "      9193--9196.\n",
            "      -- Size of data set: only 369 instances (at that point in time)\n",
            "      -- Collected classification results: 1 trial only\n",
            "      -- Two pairs of parallel hyperplanes were found to be consistent with\n",
            "         50% of the data\n",
            "         -- Accuracy on remaining 50% of dataset: 93.5%\n",
            "      -- Three pairs of parallel hyperplanes were found to be consistent with\n",
            "         67% of data\n",
            "         -- Accuracy on remaining 33% of dataset: 95.9%\n",
            "\n",
            "   2. Zhang,~J. (1992). Selecting typical instances in instance-based\n",
            "      learning.  In {\\it Proceedings of the Ninth International Machine\n",
            "      Learning Conference} (pp. 470--479).  Aberdeen, Scotland: Morgan\n",
            "      Kaufmann.\n",
            "      -- Size of data set: only 369 instances (at that point in time)\n",
            "      -- Applied 4 instance-based learning algorithms \n",
            "      -- Collected classification results averaged over 10 trials\n",
            "      -- Best accuracy result: \n",
            "         -- 1-nearest neighbor: 93.7%\n",
            "         -- trained on 200 instances, tested on the other 169\n",
            "      -- Also of interest:\n",
            "         -- Using only typical instances: 92.2% (storing only 23.1 instances)\n",
            "         -- trained on 200 instances, tested on the other 169\n",
            "\n",
            "4. Relevant Information:\n",
            "\n",
            "   Samples arrive periodically as Dr. Wolberg reports his clinical cases.\n",
            "   The database therefore reflects this chronological grouping of the data.\n",
            "   This grouping information appears immediately below, having been removed\n",
            "   from the data itself:\n",
            "\n",
            "     Group 1: 367 instances (January 1989)\n",
            "     Group 2:  70 instances (October 1989)\n",
            "     Group 3:  31 instances (February 1990)\n",
            "     Group 4:  17 instances (April 1990)\n",
            "     Group 5:  48 instances (August 1990)\n",
            "     Group 6:  49 instances (Updated January 1991)\n",
            "     Group 7:  31 instances (June 1991)\n",
            "     Group 8:  86 instances (November 1991)\n",
            "     -----------------------------------------\n",
            "     Total:   699 points (as of the donated datbase on 15 July 1992)\n",
            "\n",
            "   Note that the results summarized above in Past Usage refer to a dataset\n",
            "   of size 369, while Group 1 has only 367 instances.  This is because it\n",
            "   originally contained 369 instances; 2 were removed.  The following\n",
            "   statements summarizes changes to the original Group 1's set of data:\n",
            "\n",
            "   #####  Group 1 : 367 points: 200B 167M (January 1989)\n",
            "   #####  Revised Jan 10, 1991: Replaced zero bare nuclei in 1080185 & 1187805\n",
            "   #####  Revised Nov 22,1991: Removed 765878,4,5,9,7,10,10,10,3,8,1 no record\n",
            "   #####                  : Removed 484201,2,7,8,8,4,3,10,3,4,1 zero epithelial\n",
            "   #####                  : Changed 0 to 1 in field 6 of sample 1219406\n",
            "   #####                  : Changed 0 to 1 in field 8 of following sample:\n",
            "   #####                  : 1182404,2,3,1,1,1,2,0,1,1,1\n",
            "\n",
            "5. Number of Instances: 699 (as of 15 July 1992)\n",
            "\n",
            "6. Number of Attributes: 10 plus the class attribute\n",
            "\n",
            "7. Attribute Information: (class attribute has been moved to last column)\n",
            "\n",
            "   #  Attribute                     Domain\n",
            "   -- -----------------------------------------\n",
            "   1. Sample code number            id number\n",
            "   2. Clump Thickness               1 - 10\n",
            "   3. Uniformity of Cell Size       1 - 10\n",
            "   4. Uniformity of Cell Shape      1 - 10\n",
            "   5. Marginal Adhesion             1 - 10\n",
            "   6. Single Epithelial Cell Size   1 - 10\n",
            "   7. Bare Nuclei                   1 - 10\n",
            "   8. Bland Chromatin               1 - 10\n",
            "   9. Normal Nucleoli               1 - 10\n",
            "  10. Mitoses                       1 - 10\n",
            "  11. Class:                        (2 for benign, 4 for malignant)\n",
            "\n",
            "8. Missing attribute values: 16\n",
            "\n",
            "   There are 16 instances in Groups 1 to 6 that contain a single missing \n",
            "   (i.e., unavailable) attribute value, now denoted by \"?\".  \n",
            "\n",
            "9. Class distribution:\n",
            " \n",
            "   Benign: 458 (65.5%)\n",
            "   Malignant: 241 (34.5%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yixin's Data Formating \n",
        "    It will all collapse in a single place"
      ],
      "metadata": {
        "id": "kjUJDI5GclMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_name = ['Samplecodenumber','ClumpThickness','UniformityofCellSize','UniformityofCellShape',\n",
        "            'MarginalAdhesion','SingleEpithelialCellSize','BareNuclei',\n",
        "            'BlandChromatin','NormalNucleoli','Mitoses','Class']\n",
        "cancerdata = pd.read_csv('/content/tumor.csv', low_memory=False,names=col_name)\n",
        "cancerdata.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "XmFtvpCrclgD",
        "outputId": "9881e240-d686-4e96-bc02-a2257c153f83"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-09e24545-8f07-4102-ba7f-92e0dff74b34\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Samplecodenumber</th>\n",
              "      <th>ClumpThickness</th>\n",
              "      <th>UniformityofCellSize</th>\n",
              "      <th>UniformityofCellShape</th>\n",
              "      <th>MarginalAdhesion</th>\n",
              "      <th>SingleEpithelialCellSize</th>\n",
              "      <th>BareNuclei</th>\n",
              "      <th>BlandChromatin</th>\n",
              "      <th>NormalNucleoli</th>\n",
              "      <th>Mitoses</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sample code number</td>\n",
              "      <td>Clump Thickness</td>\n",
              "      <td>Uniformity of Cell Size</td>\n",
              "      <td>Uniformity of Cell Shape</td>\n",
              "      <td>Marginal Adhesion</td>\n",
              "      <td>Single Epithelial Cell Size</td>\n",
              "      <td>Bare Nuclei</td>\n",
              "      <td>Bland Chromatin</td>\n",
              "      <td>Normal Nucleoli</td>\n",
              "      <td>Mitoses</td>\n",
              "      <td>Class</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000025</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1002945</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1015425</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1016277</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09e24545-8f07-4102-ba7f-92e0dff74b34')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-09e24545-8f07-4102-ba7f-92e0dff74b34 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-09e24545-8f07-4102-ba7f-92e0dff74b34');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     Samplecodenumber   ClumpThickness  ...  Mitoses  Class\n",
              "0  Sample code number  Clump Thickness  ...  Mitoses  Class\n",
              "1             1000025                5  ...        1      2\n",
              "2             1002945                5  ...        1      2\n",
              "3             1015425                3  ...        1      2\n",
              "4             1016277                6  ...        1      2\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop the column that represent ID\n",
        "cancerdata = cancerdata.drop(['Samplecodenumber'], 1)\n",
        "# Check the loan status and distinct the target value.\n",
        "cancerdata['Class'].value_counts()\n",
        "#Select the finished loan including repaid and late, delete the 'current' loan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_J6Oam5c7vP",
        "outputId": "34258e60-ee31-4b5c-da9f-5b5879a78b96"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2        444\n",
              "4        239\n",
              "Class      1\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Select the finished loan including repaid and late, delete the 'current' loan\n",
        "Benign = cancerdata[(cancerdata.Class == 2) ].sample(240).index\n",
        "Malignant = cancerdata[(cancerdata.Class == 4) ].sample(240).index\n",
        "cancer = cancerdata.loc[Benign|Malignant]\n",
        "cancer = cancer.reset_index(drop=True)\n",
        "cancer['classes'] = cancer.Class.map({2:0,4:1})\n",
        "cancer = cancer.drop(['Class'], 1)\n",
        "cancer.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "xGRuBeaUdxH6",
        "outputId": "d2d20f37-6fbe-474d-96de-16d133a06bd4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-102ad8c86437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Select the finished loan including repaid and late, delete the 'current' loan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mBenign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancerdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcancerdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClass\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m240\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mMalignant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancerdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcancerdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClass\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m240\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcancer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancerdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBenign\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0mMalignant\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcancer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis)\u001b[0m\n\u001b[1;32m   4993\u001b[0m             )\n\u001b[1;32m   4994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4995\u001b[0;31m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4996\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: a must be greater than 0 unless no samples are taken"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/tumor.csv')"
      ],
      "metadata": {
        "id": "3_zd1JTRCiRM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haiRAO59C1C0",
        "outputId": "eff76389-441f-4cd2-81b8-8a345c882354"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 683 entries, 0 to 682\n",
            "Data columns (total 11 columns):\n",
            " #   Column                       Non-Null Count  Dtype\n",
            "---  ------                       --------------  -----\n",
            " 0   Sample code number           683 non-null    int64\n",
            " 1   Clump Thickness              683 non-null    int64\n",
            " 2   Uniformity of Cell Size      683 non-null    int64\n",
            " 3   Uniformity of Cell Shape     683 non-null    int64\n",
            " 4   Marginal Adhesion            683 non-null    int64\n",
            " 5   Single Epithelial Cell Size  683 non-null    int64\n",
            " 6   Bare Nuclei                  683 non-null    int64\n",
            " 7   Bland Chromatin              683 non-null    int64\n",
            " 8   Normal Nucleoli              683 non-null    int64\n",
            " 9   Mitoses                      683 non-null    int64\n",
            " 10  Class                        683 non-null    int64\n",
            "dtypes: int64(11)\n",
            "memory usage: 58.8 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a classifier"
      ],
      "metadata": {
        "id": "KF62ewDEDA_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting X & Y\n"
      ],
      "metadata": {
        "id": "E_HVCxZiW8E8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO \n",
        "# have to randomize the data first\n",
        "# After that have to divide in X and Y \n",
        "\n",
        "## Target Values \n",
        "X = df.drop(columns=['Class'])\n",
        "X = np.array(X)\n",
        "print(X.shape)\n",
        "\n",
        "## Label Values\n",
        "Y = df['Class']\n",
        "Y = np.array(Y)\n",
        "print(Y.shape)\n",
        "\n",
        "# for i in range(0,len(Y)):\n",
        "#     if Y[i] == 4:\n",
        "#         Y[i] = 1\n",
        "#     else:\n",
        "#         Y[i]=0\n",
        "# # Storing the number of rows and columns in X\n",
        "# rows , cols = X.shape\n",
        "## Making 2 and 4 to 0 and 1 to get result according to sigmoid function\n",
        "\n",
        "Y = list(map(lambda x : 0 if x==2 else 1, Y))\n",
        "Y = np.array(Y)\n",
        "\n",
        "# X_train = X[0:513]\n",
        "# X_test = X[513:]\n",
        "# Y_train = Y[0:513]\n",
        "# Y_test = Y[513:]\n",
        "# print(X_train.shape,Y_train.shape)\n",
        "# print(X_test.shape,Y_test.shape)\n",
        "\n",
        "#This normalization doesnot work \n",
        "\n",
        "#X=(X-X.mean())/X.std()\n",
        "\n",
        "X=(X-X.min())/(X.max()-X.min())\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=15)\n",
        "\n",
        "\n",
        "# Using the inbuilt one gives 100% accuracy because it is dumbdata \n",
        "# #Standardizing the data.\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_test = scaler.transform(X_test)\n",
        "# X_train.shape, y_train.shape, X_test.shape, y_test.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGU0KqqjW8Xw",
        "outputId": "de4c2823-1609-44ac-efd4-5bc0002919c6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(683, 10)\n",
            "(683,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intialize the weights"
      ],
      "metadata": {
        "id": "MhCmi37GYpRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(dim):\n",
        "    ''' In this function, we will initialize our weights and bias'''\n",
        "    # Here dimenstion refer to the number of the attributes in the data\n",
        "    w = np.zeros(shape=len(dim))\n",
        "    b = 0\n",
        "    return w,b"
      ],
      "metadata": {
        "id": "pRqb-07xDD6m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Just to check wether the function is working fine\n",
        "dim=X_train[0]\n",
        "w,b = initialize_weights(dim)\n",
        "print('w =',(w))\n",
        "print('b =',b)\n",
        "print('w',np.sum(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z96iV0wuDD9Y",
        "outputId": "a5056957-03d8-4262-dfd5-4d9393fa85e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "b = 0\n",
            "w 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grader Function - Weights\n",
        "    Check whether things are working fine or not"
      ],
      "metadata": {
        "id": "Yy9l9puJZHHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Grader Function\n",
        "## Dont run this cell untill and unless you want to check wether everything is fine or not \n",
        "w,b = initialize_weights(dim)\n",
        "def grader_weights(w,b):\n",
        "  assert((len(w)==len(dim)) and b==0 and np.sum(w)==0.0)\n",
        "  return True\n",
        "grader_weights(w,b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Nzv0I31DEAT",
        "outputId": "64da6018-caad-4d75-e9c7-f1302f0c194d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Sigmoid\n",
        "\n",
        "![Sigmoid Function](https://www.gstatic.com/education/formulas2/397133473/en/sigmoid_function.svg)"
      ],
      "metadata": {
        "id": "O8piuV5rZUtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    ''' In this function, we will return sigmoid of z'''\n",
        "    # compute sigmoid(z) and return\n",
        "    return 1/(1+np.exp(-z))"
      ],
      "metadata": {
        "id": "CWYiNX80ZUCS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Log Loss \n",
        "![Log Loss](https://miro.medium.com/max/1192/1*wilGXrItaMAJmZNl6RJq9Q.png)"
      ],
      "metadata": {
        "id": "a_7VxHj6bvEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(y_labels,y_predicted):\n",
        "    '''This function will return the log loss of the function'''\n",
        "    loss = -1 * (np.sum((y_labels * np.log10(y_predicted))+ \\\n",
        "                      ((1-y_labels)*np.log10(1-y_predicted))))/len(y_labels)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "UAnfSyDrDECM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grader Function log_loss"
      ],
      "metadata": {
        "id": "LzAWM5XFejHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grader_logloss(true,pred):\n",
        "    loss=log_loss(true,pred)\n",
        "    assert(loss==0.07644900402910389)\n",
        "    return True\n",
        "true=np.array([1,1,0,1,0])\n",
        "pred=np.array([0.9,0.8,0.1,0.8,0.2])\n",
        "grader_logloss(true,pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emvDMysqDEE9",
        "outputId": "1368a5ff-f72f-4ea1-9cda-1d54a394b212"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient with respect w (dw)\n",
        "![Differntiation of cost function wrt to w](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-35e8bb42947bd888580c2a8a9fe8fe0e_l3.svg)"
      ],
      "metadata": {
        "id": "9ZR6i_Qef-4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_dw(x,y,w,b,alpha,N):\n",
        "    '''In this function, we will compute the gardient w.r.to w '''\n",
        "    # Calculcating the graindent of weighted vectors\n",
        "    return x * (y - sigmoid(np.dot(w, x) + b)) - alpha/N*w"
      ],
      "metadata": {
        "id": "_haqFGTcgAg-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grader_dw(x,y,w,b,alpha,N):\n",
        "    grad_dw=gradient_dw(x,y,w,b,alpha,N)\n",
        "    assert(np.sum(grad_dw)==2.7259648199999997)\n",
        "    return True\n",
        "\n",
        "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
        "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725])\n",
        "\n",
        "grad_y=0\n",
        "grad_w,grad_b=initialize_weights(dim)\n",
        "alpha=0.0001\n",
        "N=len(X)\n",
        "grader_dw(grad_x,grad_y,grad_w,grad_b,alpha,N)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c9X9RsKgBkg",
        "outputId": "a258d8b1-b164-4966-82be-2b62290f183d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute gradient w.r.t 'b'"
      ],
      "metadata": {
        "id": "IVFmsUW3stPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_db(x,y,w,b):\n",
        "    '''In this function, we will compute gradient w.r.to b '''\n",
        "    # Calculating the gradient of bais\n",
        "    return y - sigmoid(np.dot(w, x) + b)"
      ],
      "metadata": {
        "id": "AERrDcGGstfy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grader_db(x,y,w,b):\n",
        "  grad_db=gradient_db(x,y,w,b)\n",
        "  assert(grad_db==-0.5)\n",
        "  return True\n",
        "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
        "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725])\n",
        "grad_y=0\n",
        "grad_w,grad_b=initialize_weights(grad_x)\n",
        "alpha=0.0001\n",
        "N=len(X_train)\n",
        "grader_db(grad_x,grad_y,grad_w,grad_b)\n",
        "True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9csvJwjHWtF",
        "outputId": "fb29e394-26ed-4886-d7fd-cad71d383752"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Function"
      ],
      "metadata": {
        "id": "CgkNsFilIy0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0,p):\n",
        "    ''' In this function, we will implement logistic regression'''\n",
        "    \n",
        "    w,b = initialize_weights(X_train[0]) # intilize weight vectors\n",
        "    same_loss_counter = 0\n",
        "    N = len(X_train)\n",
        "    train_loss , test_loss = [],[]\n",
        "    part_no = 0\n",
        "    part_size = 25\n",
        "    ctr = 0\n",
        "    n = len(X_train)\n",
        "    \n",
        "    # Loop to traveres in epoches\n",
        "    for i in tqdm(range(0,epochs)):\n",
        "        # Loop to access data point in the \n",
        "        for j in range(part_size):\n",
        "            \n",
        "            # Calculating gradient of w and adding it to the existing one    \n",
        "            w = w + eta0*gradient_dw(X_train[(j+part_no)%n], y_train[(j+part_no)%n],w, b, alpha, len(X_train))\n",
        "            \n",
        "            #Calculating gradient of b and adding it to the existing one\n",
        "            b = b + eta0*gradient_db(X_train[(j+part_no)%n], y_train[(j+part_no)%n], w, b)\n",
        "        \n",
        "\n",
        "        part_no = (part_no + part_size)%n # To updtae the new part\n",
        "\n",
        "        #Predicting the traing data in comparison of the the xtrain\n",
        "        y_pred_train = np.array([sigmoid(np.dot(w, x)+b) for x in X_train])\n",
        "        \n",
        "        #Predicting the test data in comaprison of the xtest\n",
        "        y_pred_test = np.array([sigmoid(np.dot(w, x)+b) for x in X_test])\n",
        "\n",
        "        #Calculating the loss on for training data\n",
        "        loss = log_loss(y_train,y_pred_train)\n",
        "        train_loss.append(loss)\n",
        "        \n",
        "        #Calculatig the loss onfor testing data\n",
        "        loss = log_loss(y_test,y_pred_test)\n",
        "        test_loss.append(loss)\n",
        "\n",
        "        ## Printing values\n",
        "        print('\\n-- Epoch no(iteration no) ', i+1,'\\n Train data set : ')\n",
        "        #print('Actual values: ', y_train ,'\\n Predicted Values : ', y_pred_train)\n",
        "        #print('Test data set :') \n",
        "       # print('Actual values: ', y_test, '\\nPredicated Values : ', y_pred_test)\n",
        "        print('W intercept: {}, B intercept: {}, Train loss: {}, Test loss: {}'\\\n",
        "              .format(w, b, train_loss[i], test_loss[i]))\n",
        "    return w,b,train_loss,test_loss"
      ],
      "metadata": {
        "id": "2aLSyLZRHKun"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha=0.0001\n",
        "eta0=0.01\n",
        "N=len(X_train)\n",
        "epochs=100\n",
        "p = 2\n",
        "w,b,train_loss,test_loss=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0,p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V56xhKCJfbA",
        "outputId": "d067f92e-9b1f-474d-adad-7e8c9a1f80e7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 30/100 [00:00<00:00, 150.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Epoch no(iteration no)  1 \n",
            " Train data set : \n",
            "W intercept: [-4.51273115e-03 -5.04383975e-10  1.61394765e-08  1.46280228e-08\n",
            "  2.06342799e-08  8.70868028e-09  2.24681050e-08  1.17047969e-08\n",
            "  1.46351933e-08  3.02465977e-09], B intercept: -0.043980536945789704, Train loss: 0.2979371939204877, Test loss: 0.29915889259917644\n",
            "\n",
            "-- Epoch no(iteration no)  2 \n",
            " Train data set : \n",
            "W intercept: [-6.58039172e-03  1.45806862e-08  3.87261799e-08  3.15750685e-08\n",
            "  3.68240507e-08  2.04969607e-08  4.55269396e-08  2.49851182e-08\n",
            "  3.04857055e-08  1.03198678e-08], B intercept: -0.07557873965716028, Train loss: 0.2958535014417353, Test loss: 0.29795159877579946\n",
            "\n",
            "-- Epoch no(iteration no)  3 \n",
            " Train data set : \n",
            "W intercept: [-1.00414995e-02  2.32208996e-08  5.79536187e-08  5.23714143e-08\n",
            "  5.42744118e-08  3.12264138e-08  7.23890185e-08  3.55007973e-08\n",
            "  4.51487468e-08  1.46467654e-08], B intercept: -0.10468648088068493, Train loss: 0.2940223915698621, Test loss: 0.2969297722476129\n",
            "\n",
            "-- Epoch no(iteration no)  4 \n",
            " Train data set : \n",
            "W intercept: [-9.77976269e-03  3.91430579e-08  7.51920129e-08  7.20382253e-08\n",
            "  6.63743732e-08  4.43366220e-08  1.05492668e-07  4.99058277e-08\n",
            "  6.16726444e-08  1.74053296e-08], B intercept: -0.10379966962775367, Train loss: 0.2940776454283117, Test loss: 0.296960176419943\n",
            "\n",
            "-- Epoch no(iteration no)  5 \n",
            " Train data set : \n",
            "W intercept: [-1.34385784e-02  4.63129572e-08  9.61494659e-08  9.07996487e-08\n",
            "  7.85641459e-08  5.06741756e-08  1.22532686e-07  5.94535367e-08\n",
            "  7.72685931e-08  2.13627223e-08], B intercept: -0.1509393279898632, Train loss: 0.2913154650190119, Test loss: 0.2955063419280998\n",
            "\n",
            "-- Epoch no(iteration no)  6 \n",
            " Train data set : \n",
            "W intercept: [-1.86707669e-02  5.08761526e-08  1.05625501e-07  1.03233710e-07\n",
            "  8.85243599e-08  5.56124620e-08  1.42169652e-07  6.30027775e-08\n",
            "  9.20026678e-08  2.61956499e-08], B intercept: -0.20509091432963758, Train loss: 0.2884373551409098, Test loss: 0.2941326551894931\n",
            "\n",
            "-- Epoch no(iteration no)  7 \n",
            " Train data set : \n",
            "W intercept: [-2.15955739e-02  6.62425647e-08  1.20044553e-07  1.17458054e-07\n",
            "  1.01536053e-07  6.29993614e-08  1.56593215e-07  6.86662427e-08\n",
            "  1.04331753e-07  3.28391247e-08], B intercept: -0.23623239470900206, Train loss: 0.2869275352334073, Test loss: 0.29348798466315085\n",
            "\n",
            "-- Epoch no(iteration no)  8 \n",
            " Train data set : \n",
            "W intercept: [-2.43411064e-02  7.77190485e-08  1.41510721e-07  1.38790377e-07\n",
            "  1.13703728e-07  7.62328586e-08  1.77508030e-07  7.89357640e-08\n",
            "  1.21069724e-07  4.00099126e-08], B intercept: -0.26488177908755484, Train loss: 0.2856313813293565, Test loss: 0.2929878758500425\n",
            "\n",
            "-- Epoch no(iteration no)  9 \n",
            " Train data set : \n",
            "W intercept: [-3.03782008e-02  7.97261943e-08  1.53687723e-07  1.45202621e-07\n",
            "  1.28008811e-07  7.89650871e-08  1.95017923e-07  8.18541078e-08\n",
            "  1.30672629e-07  4.59794014e-08], B intercept: -0.3017319230015452, Train loss: 0.2840855126196293, Test loss: 0.2924692824768823\n",
            "\n",
            "-- Epoch no(iteration no)  10 \n",
            " Train data set : \n",
            "W intercept: [-3.25528679e-02  9.36472826e-08  1.68513829e-07  1.58302740e-07\n",
            "  1.41329398e-07  8.86616574e-08  2.12657948e-07  9.31973413e-08\n",
            "  1.48680538e-07  4.76943282e-08], B intercept: -0.3275491725432661, Train loss: 0.28309779717531786, Test loss: 0.2921986622161385\n",
            "\n",
            "-- Epoch no(iteration no)  11 \n",
            " Train data set : \n",
            "W intercept: [-3.46078958e-02  1.07508806e-07  1.85439568e-07  1.74164731e-07\n",
            "  1.51010808e-07  1.00070634e-07  2.33425904e-07  1.05534019e-07\n",
            "  1.59116049e-07  5.24443465e-08], B intercept: -0.3512193498017622, Train loss: 0.28225474567622777, Test loss: 0.29201318487292965\n",
            "\n",
            "-- Epoch no(iteration no)  12 \n",
            " Train data set : \n",
            "W intercept: [-3.55927623e-02  1.24071828e-07  2.07983010e-07  1.93591677e-07\n",
            "  1.73115755e-07  1.10994008e-07  2.61583201e-07  1.22130676e-07\n",
            "  1.83155681e-07  6.29838160e-08], B intercept: -0.3634715340378558, Train loss: 0.2818421122748043, Test loss: 0.29194083374104624\n",
            "\n",
            "-- Epoch no(iteration no)  13 \n",
            " Train data set : \n",
            "W intercept: [-3.73824359e-02  1.36580288e-07  2.22260268e-07  2.08915585e-07\n",
            "  1.84474078e-07  1.17986696e-07  2.80792704e-07  1.34557200e-07\n",
            "  2.02395888e-07  6.38712887e-08], B intercept: -0.3848013089070063, Train loss: 0.2811616022919982, Test loss: 0.291852840107122\n",
            "\n",
            "-- Epoch no(iteration no)  14 \n",
            " Train data set : \n",
            "W intercept: [-3.84527824e-02  1.52478353e-07  2.53882451e-07  2.39631719e-07\n",
            "  2.03898923e-07  1.35850558e-07  3.09952204e-07  1.52796485e-07\n",
            "  2.21804996e-07  6.69553538e-08], B intercept: -0.3864427802422633, Train loss: 0.2811084255602072, Test loss: 0.2918465152214552\n",
            "\n",
            "-- Epoch no(iteration no)  15 \n",
            " Train data set : \n",
            "W intercept: [-4.15556419e-02  1.57602225e-07  2.69565681e-07  2.52465589e-07\n",
            "  2.17202584e-07  1.41105532e-07  3.29208334e-07  1.65374573e-07\n",
            "  2.36756381e-07  6.81556728e-08], B intercept: -0.42611525588328264, Train loss: 0.2799785873021676, Test loss: 0.29181851153930277\n",
            "\n",
            "-- Epoch no(iteration no)  16 \n",
            " Train data set : \n",
            "W intercept: [-4.28633829e-02  1.76515852e-07  2.87719367e-07  2.71114235e-07\n",
            "  2.33251625e-07  1.48792885e-07  3.55392990e-07  1.82689526e-07\n",
            "  2.58623123e-07  6.96800324e-08], B intercept: -0.43417186765410626, Train loss: 0.27976760483337176, Test loss: 0.2918322198677241\n",
            "\n",
            "-- Epoch no(iteration no)  17 \n",
            " Train data set : \n",
            "W intercept: [-4.42522637e-02  1.79206447e-07  3.08241709e-07  2.90990151e-07\n",
            "  2.45280013e-07  1.59970163e-07  3.71034425e-07  1.96940442e-07\n",
            "  2.77078224e-07  7.18349781e-08], B intercept: -0.46149692054271024, Train loss: 0.27911057847704257, Test loss: 0.29193313890080214\n",
            "\n",
            "-- Epoch no(iteration no)  18 \n",
            " Train data set : \n",
            "W intercept: [-4.52993832e-02  1.88236145e-07  3.22317721e-07  3.06158260e-07\n",
            "  2.63875379e-07  1.71353467e-07  3.95455419e-07  2.09981328e-07\n",
            "  2.94646060e-07  8.05436619e-08], B intercept: -0.4777437574961865, Train loss: 0.2787561761871481, Test loss: 0.2920297088541931\n",
            "\n",
            "-- Epoch no(iteration no)  19 \n",
            " Train data set : \n",
            "W intercept: [-4.65787357e-02  2.06002870e-07  3.41431137e-07  3.26102881e-07\n",
            "  2.83598722e-07  1.82909384e-07  4.22863093e-07  2.21839275e-07\n",
            "  3.08623118e-07  9.48821393e-08], B intercept: -0.4925717335030613, Train loss: 0.27845589716788866, Test loss: 0.29214147460642914\n",
            "\n",
            "-- Epoch no(iteration no)  20 \n",
            " Train data set : \n",
            "W intercept: [-4.37225963e-02  2.38652985e-07  3.71750081e-07  3.56296642e-07\n",
            "  3.07803715e-07  1.99806561e-07  4.53296356e-07  2.44827656e-07\n",
            "  3.34524931e-07  1.10944552e-07], B intercept: -0.4585578287816212, Train loss: 0.27917843992639996, Test loss: 0.2919189534523025\n",
            "\n",
            "-- Epoch no(iteration no)  21 \n",
            " Train data set : \n",
            "W intercept: [-4.61646746e-02  2.40233656e-07  3.91218934e-07  3.76857937e-07\n",
            "  3.25326986e-07  2.07655834e-07  4.79004838e-07  2.59774250e-07\n",
            "  3.52895942e-07  1.13462565e-07], B intercept: -0.48461196795897754, Train loss: 0.27861365340960625, Test loss: 0.29207841685238306\n",
            "\n",
            "-- Epoch no(iteration no)  22 \n",
            " Train data set : \n",
            "W intercept: [-4.69842690e-02  2.56184095e-07  4.15290057e-07  3.96107276e-07\n",
            "  3.52701407e-07  2.22509941e-07  5.06201591e-07  2.78229474e-07\n",
            "  3.74823786e-07  1.23318672e-07], B intercept: -0.4895729414440802, Train loss: 0.27851335175021275, Test loss: 0.2921165143088245\n",
            "\n",
            "-- Epoch no(iteration no)  23 \n",
            " Train data set : \n",
            "W intercept: [-4.86386401e-02  2.70770629e-07  4.41446700e-07  4.18292035e-07\n",
            "  3.72127066e-07  2.39382997e-07  5.34131365e-07  2.90566995e-07\n",
            "  3.93891175e-07  1.31097281e-07], B intercept: -0.5041292766245502, Train loss: 0.27823547290138106, Test loss: 0.2922436973702188\n",
            "\n",
            "-- Epoch no(iteration no)  24 \n",
            " Train data set : \n",
            "W intercept: [-4.64752543e-02  3.01307162e-07  4.68388918e-07  4.50060034e-07\n",
            "  3.93538110e-07  2.61465113e-07  5.83055031e-07  3.14334587e-07\n",
            "  4.22018499e-07  1.36168495e-07], B intercept: -0.4689965488108406, Train loss: 0.27893953299264296, Test loss: 0.29197265377035403\n",
            "\n",
            "-- Epoch no(iteration no)  25 \n",
            " Train data set : \n",
            "W intercept: [-4.72679287e-02  3.14098521e-07  4.87364044e-07  4.68818715e-07\n",
            "  4.08366190e-07  2.67847955e-07  6.04253196e-07  3.26392459e-07\n",
            "  4.35440434e-07  1.36807909e-07], B intercept: -0.48428270815262114, Train loss: 0.27861785302241826, Test loss: 0.2920750281121075\n",
            "\n",
            "-- Epoch no(iteration no)  26 \n",
            " Train data set : \n",
            "W intercept: [-4.98132913e-02  3.25349472e-07  5.03165097e-07  4.87222226e-07\n",
            "  4.20288730e-07  2.79273796e-07  6.27405137e-07  3.34819066e-07\n",
            "  4.49695820e-07  1.43278084e-07], B intercept: -0.5087144504945188, Train loss: 0.27815118859208504, Test loss: 0.2922879165435869\n",
            "\n",
            "-- Epoch no(iteration no)  27 \n",
            " Train data set : \n",
            "W intercept: [-5.19134832e-02  3.41712550e-07  5.17150133e-07  5.02909805e-07\n",
            "  4.30610480e-07  2.85641928e-07  6.47569491e-07  3.41658662e-07\n",
            "  4.64715209e-07  1.47944755e-07], B intercept: -0.5312183562969821, Train loss: 0.27777642190941204, Test loss: 0.2925387756891696\n",
            "\n",
            "-- Epoch no(iteration no)  28 \n",
            " Train data set : \n",
            "W intercept: [-5.54376562e-02  3.53562414e-07  5.34271631e-07  5.16931227e-07\n",
            "  4.44822917e-07  2.96134104e-07  6.61364197e-07  3.49111890e-07\n",
            "  4.78129309e-07  1.55990372e-07], B intercept: -0.5622275765258016, Train loss: 0.27734412948041975, Test loss: 0.2929695000121587\n",
            "\n",
            "-- Epoch no(iteration no)  29 \n",
            " Train data set : \n",
            "W intercept: [-5.83207556e-02  3.64293357e-07  5.54628494e-07  5.37781667e-07\n",
            "  4.62138792e-07  3.10353060e-07  6.87844284e-07  3.62530063e-07\n",
            "  4.95164147e-07  1.65978410e-07], B intercept: -0.5725263243425105, Train loss: 0.27721977459542757, Test loss: 0.29313420484379027\n",
            "\n",
            "-- Epoch no(iteration no)  30 \n",
            " Train data set : \n",
            "W intercept: [-5.87777182e-02  3.83382461e-07  5.79175786e-07  5.53417720e-07\n",
            "  4.81113351e-07  3.21819378e-07  7.11287320e-07  3.74067652e-07\n",
            "  5.10646257e-07  1.69793970e-07], B intercept: -0.5721506336662039, Train loss: 0.2772232791850323, Test loss: 0.293127958890166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 64%|██████▍   | 64/100 [00:00<00:00, 160.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Epoch no(iteration no)  31 \n",
            " Train data set : \n",
            "W intercept: [-6.06887208e-02  3.94352781e-07  5.87055762e-07  5.63892524e-07\n",
            "  4.90178695e-07  3.29752138e-07  7.34011767e-07  3.85134044e-07\n",
            "  5.25268495e-07  1.72665812e-07], B intercept: -0.5916666364268948, Train loss: 0.2770220595641304, Test loss: 0.2934695182180751\n",
            "\n",
            "-- Epoch no(iteration no)  32 \n",
            " Train data set : \n",
            "W intercept: [-6.18392386e-02  4.12206301e-07  6.10952117e-07  5.83098992e-07\n",
            "  5.07643914e-07  3.44715254e-07  7.56593922e-07  4.04965951e-07\n",
            "  5.43476049e-07  1.79575808e-07], B intercept: -0.5999235838555453, Train loss: 0.27694808751908784, Test loss: 0.2936256820703353\n",
            "\n",
            "-- Epoch no(iteration no)  33 \n",
            " Train data set : \n",
            "W intercept: [-6.19039934e-02  4.32023687e-07  6.27780725e-07  6.01725683e-07\n",
            "  5.24820544e-07  3.56102198e-07  7.75290664e-07  4.17502525e-07\n",
            "  5.65782404e-07  1.87257561e-07], B intercept: -0.6084321433757279, Train loss: 0.27688043995473427, Test loss: 0.2937935906879079\n",
            "\n",
            "-- Epoch no(iteration no)  34 \n",
            " Train data set : \n",
            "W intercept: [-6.08336858e-02  4.55269678e-07  6.61824628e-07  6.34016711e-07\n",
            "  5.45930991e-07  3.70507985e-07  8.12299985e-07  4.42454305e-07\n",
            "  5.91771362e-07  1.91088858e-07], B intercept: -0.5868403760821768, Train loss: 0.2770673849384728, Test loss: 0.29338148984049933\n",
            "\n",
            "-- Epoch no(iteration no)  35 \n",
            " Train data set : \n",
            "W intercept: [-6.18030991e-02  4.69347080e-07  6.89342586e-07  6.61916400e-07\n",
            "  5.65257932e-07  3.88612444e-07  8.39220394e-07  4.58792285e-07\n",
            "  6.10956377e-07  1.92530875e-07], B intercept: -0.5956277713825691, Train loss: 0.2769849769477644, Test loss: 0.2935436386522387\n",
            "\n",
            "-- Epoch no(iteration no)  36 \n",
            " Train data set : \n",
            "W intercept: [-6.40635775e-02  4.79294775e-07  7.02525134e-07  6.73092392e-07\n",
            "  5.75055913e-07  3.90440697e-07  8.53382711e-07  4.73050047e-07\n",
            "  6.27629908e-07  1.94916208e-07], B intercept: -0.6231270883344501, Train loss: 0.27677826975911723, Test loss: 0.2941011753149781\n",
            "\n",
            "-- Epoch no(iteration no)  37 \n",
            " Train data set : \n",
            "W intercept: [-6.54589498e-02  4.87684115e-07  7.20665604e-07  6.91159125e-07\n",
            "  5.93409696e-07  4.00098631e-07  8.83883575e-07  4.88932902e-07\n",
            "  6.50129315e-07  1.95143823e-07], B intercept: -0.6302327143343818, Train loss: 0.27673632168462026, Test loss: 0.2942578720384972\n",
            "\n",
            "-- Epoch no(iteration no)  38 \n",
            " Train data set : \n",
            "W intercept: [-6.35607377e-02  5.05366345e-07  7.49217484e-07  7.18337743e-07\n",
            "  6.17631708e-07  4.22012425e-07  9.13492894e-07  5.14221758e-07\n",
            "  6.78743466e-07  2.01425911e-07], B intercept: -0.6167681546466761, Train loss: 0.27681931396519605, Test loss: 0.29396552121421965\n",
            "\n",
            "-- Epoch no(iteration no)  39 \n",
            " Train data set : \n",
            "W intercept: [-6.53808629e-02  5.18813726e-07  7.63195026e-07  7.32373278e-07\n",
            "  6.37062931e-07  4.30483708e-07  9.31098932e-07  5.22352187e-07\n",
            "  6.83484004e-07  2.12159241e-07], B intercept: -0.6431625170145897, Train loss: 0.27667553967726555, Test loss: 0.2945548056144379\n",
            "\n",
            "-- Epoch no(iteration no)  40 \n",
            " Train data set : \n",
            "W intercept: [-6.37299143e-02  5.47968755e-07  7.91420480e-07  7.60070370e-07\n",
            "  6.57681321e-07  4.43457178e-07  9.71745833e-07  5.41883839e-07\n",
            "  7.08101122e-07  2.30226087e-07], B intercept: -0.6191490176484417, Train loss: 0.2768034948398463, Test loss: 0.29401583258172\n",
            "\n",
            "-- Epoch no(iteration no)  41 \n",
            " Train data set : \n",
            "W intercept: [-6.35099444e-02  5.63864307e-07  8.13704564e-07  7.84772847e-07\n",
            "  6.75092678e-07  4.57429422e-07  9.90835732e-07  5.58054205e-07\n",
            "  7.33309125e-07  2.41322151e-07], B intercept: -0.6172212206839062, Train loss: 0.2768163538720244, Test loss: 0.29397502673897874\n",
            "\n",
            "-- Epoch no(iteration no)  42 \n",
            " Train data set : \n",
            "W intercept: [-6.43748705e-02  5.72314442e-07  8.39770814e-07  8.08856937e-07\n",
            "  7.05016882e-07  4.74553541e-07  1.02434828e-06  5.79800310e-07\n",
            "  7.53344736e-07  2.49458801e-07], B intercept: -0.6148181581260467, Train loss: 0.27683159743195307, Test loss: 0.2939250037039607\n",
            "\n",
            "-- Epoch no(iteration no)  43 \n",
            " Train data set : \n",
            "W intercept: [-6.42785409e-02  5.99376363e-07  8.68966263e-07  8.32666649e-07\n",
            "  7.27400130e-07  4.92155465e-07  1.05519777e-06  5.98637030e-07\n",
            "  7.77622108e-07  2.55716522e-07], B intercept: -0.6122890814495915, Train loss: 0.27684939748977944, Test loss: 0.29387267524701577\n",
            "\n",
            "-- Epoch no(iteration no)  44 \n",
            " Train data set : \n",
            "W intercept: [-6.51526199e-02  6.16481729e-07  8.90707854e-07  8.57800584e-07\n",
            "  7.47376618e-07  5.07627935e-07  1.09041488e-06  6.13784628e-07\n",
            "  7.97632396e-07  2.61763333e-07], B intercept: -0.6093768121132753, Train loss: 0.27686950956981327, Test loss: 0.2938134399957712\n",
            "\n",
            "-- Epoch no(iteration no)  45 \n",
            " Train data set : \n",
            "W intercept: [-6.29846258e-02  6.38401098e-07  9.11023114e-07  8.79209060e-07\n",
            "  7.62465892e-07  5.25641644e-07  1.12688287e-06  6.32195417e-07\n",
            "  8.15025809e-07  2.64859921e-07], B intercept: -0.5883094296193302, Train loss: 0.2770503623621262, Test loss: 0.29340817576392436\n",
            "\n",
            "-- Epoch no(iteration no)  46 \n",
            " Train data set : \n",
            "W intercept: [-6.42416792e-02  6.51688325e-07  9.36464093e-07  9.03425708e-07\n",
            "  7.77852336e-07  5.36034916e-07  1.14773255e-06  6.46381925e-07\n",
            "  8.34610457e-07  2.69653789e-07], B intercept: -0.6070022184049939, Train loss: 0.2768884635437632, Test loss: 0.29376537752336523\n",
            "\n",
            "-- Epoch no(iteration no)  47 \n",
            " Train data set : \n",
            "W intercept: [-6.75296889e-02  6.61438850e-07  9.48231909e-07  9.19017051e-07\n",
            "  7.91393810e-07  5.43979936e-07  1.17180201e-06  6.54655730e-07\n",
            "  8.52662094e-07  2.75437875e-07], B intercept: -0.6345298343665385, Train loss: 0.27671222043908156, Test loss: 0.2943556670215128\n",
            "\n",
            "-- Epoch no(iteration no)  48 \n",
            " Train data set : \n",
            "W intercept: [-6.83484855e-02  6.84053389e-07  9.65738550e-07  9.36050007e-07\n",
            "  8.07480724e-07  5.54528717e-07  1.18996631e-06  6.63690236e-07\n",
            "  8.67718417e-07  2.83239616e-07], B intercept: -0.6408041564086935, Train loss: 0.2766826164061281, Test loss: 0.2945008850451922\n",
            "\n",
            "-- Epoch no(iteration no)  49 \n",
            " Train data set : \n",
            "W intercept: [-6.93740714e-02  7.00417164e-07  9.91183529e-07  9.61198782e-07\n",
            "  8.22600664e-07  5.72236805e-07  1.21512504e-06  6.78169009e-07\n",
            "  8.87540278e-07  2.91572603e-07], B intercept: -0.6461752929795567, Train loss: 0.2766601098168203, Test loss: 0.2946285054439049\n",
            "\n",
            "-- Epoch no(iteration no)  50 \n",
            " Train data set : \n",
            "W intercept: [-7.29819669e-02  7.08664092e-07  1.00541164e-06  9.70456266e-07\n",
            "  8.39285971e-07  5.78194830e-07  1.23573379e-06  6.83431165e-07\n",
            "  8.98822557e-07  2.98451552e-07], B intercept: -0.6610961363666448, Train loss: 0.276612237213369, Test loss: 0.2949987982480356\n",
            "\n",
            "-- Epoch no(iteration no)  51 \n",
            " Train data set : \n",
            "W intercept: [-7.34290333e-02  7.28308476e-07  1.02257482e-06  9.85138722e-07\n",
            "  8.54936946e-07  5.91499309e-07  1.25675118e-06  6.98281154e-07\n",
            "  9.19397773e-07  3.00411528e-07], B intercept: -0.6663529606828762, Train loss: 0.2766013504350029, Test loss: 0.2951340512542026\n",
            "\n",
            "-- Epoch no(iteration no)  52 \n",
            " Train data set : \n",
            "W intercept: [-7.41193285e-02  7.47605555e-07  1.04298262e-06  1.00504981e-06\n",
            "  8.66959131e-07  6.06698384e-07  1.28181675e-06  7.13728596e-07\n",
            "  9.32177701e-07  3.06073213e-07], B intercept: -0.6707288451545512, Train loss: 0.2765941388119156, Test loss: 0.2952489519150454\n",
            "\n",
            "-- Epoch no(iteration no)  53 \n",
            " Train data set : \n",
            "W intercept: [-7.27191754e-02  7.73358704e-07  1.06912502e-06  1.02846272e-06\n",
            "  8.92111461e-07  6.23139662e-07  1.31438543e-06  7.36637863e-07\n",
            "  9.59310668e-07  3.17895696e-07], B intercept: -0.6548903758896816, Train loss: 0.2766283784269698, Test loss: 0.29484280397305135\n",
            "\n",
            "-- Epoch no(iteration no)  54 \n",
            " Train data set : \n",
            "W intercept: [-7.31278163e-02  7.88781691e-07  1.08906536e-06  1.04939961e-06\n",
            "  9.06650320e-07  6.30732194e-07  1.33953782e-06  7.52352785e-07\n",
            "  9.80803043e-07  3.18881982e-07], B intercept: -0.6591619434271572, Train loss: 0.27661668119086996, Test loss: 0.2949499210647327\n",
            "\n",
            "-- Epoch no(iteration no)  55 \n",
            " Train data set : \n",
            "W intercept: [-7.37787018e-02  8.07275887e-07  1.12085652e-06  1.07993321e-06\n",
            "  9.26715999e-07  6.50188614e-07  1.36669008e-06  7.71240917e-07\n",
            "  1.00232607e-06  3.22303501e-07], B intercept: -0.6547028332950581, Train loss: 0.2766280369529257, Test loss: 0.29483880389945993\n",
            "\n",
            "-- Epoch no(iteration no)  56 \n",
            " Train data set : \n",
            "W intercept: [-7.50615959e-02  8.17471595e-07  1.13855944e-06  1.09586974e-06\n",
            "  9.41947093e-07  6.59200374e-07  1.39128750e-06  7.87940361e-07\n",
            "  1.02327687e-06  3.23771007e-07], B intercept: -0.6692126430497246, Train loss: 0.2765955719810015, Test loss: 0.2952097882306298\n",
            "\n",
            "-- Epoch no(iteration no)  57 \n",
            " Train data set : \n",
            "W intercept: [-7.57226254e-02  8.35972370e-07  1.15881231e-06  1.11563056e-06\n",
            "  9.59962634e-07  6.66835914e-07  1.41846470e-06  8.05432510e-07\n",
            "  1.04280156e-06  3.25504884e-07], B intercept: -0.673115584771953, Train loss: 0.2765901633316561, Test loss: 0.2953133626447423\n",
            "\n",
            "-- Epoch no(iteration no)  58 \n",
            " Train data set : \n",
            "W intercept: [-7.61607706e-02  8.41811982e-07  1.18081540e-06  1.13737770e-06\n",
            "  9.73301747e-07  6.79902146e-07  1.43573820e-06  8.22677554e-07\n",
            "  1.06331427e-06  3.27974556e-07], B intercept: -0.6870538601966851, Train loss: 0.2765843318295821, Test loss: 0.2956939452725578\n",
            "\n",
            "-- Epoch no(iteration no)  59 \n",
            " Train data set : \n",
            "W intercept: [-7.63821793e-02  8.54635291e-07  1.19763244e-06  1.15470140e-06\n",
            "  9.93594456e-07  6.93489568e-07  1.46270482e-06  8.38002854e-07\n",
            "  1.08286102e-06  3.37382877e-07], B intercept: -0.6907123008822986, Train loss: 0.27658586793670326, Test loss: 0.2957970607125175\n",
            "\n",
            "-- Epoch no(iteration no)  60 \n",
            " Train data set : \n",
            "W intercept: [-7.66903855e-02  8.74803677e-07  1.21825219e-06  1.17632065e-06\n",
            "  1.01518800e-06  7.07186476e-07  1.49251682e-06  8.51933348e-07\n",
            "  1.09804365e-06  3.52797248e-07], B intercept: -0.6937559003299595, Train loss: 0.27658807283263953, Test loss: 0.2958839546663103\n",
            "\n",
            "-- Epoch no(iteration no)  61 \n",
            " Train data set : \n",
            "W intercept: [-7.32317888e-02  9.11267333e-07  1.25121329e-06  1.20925425e-06\n",
            "  1.04118877e-06  7.25933080e-07  1.52592806e-06  8.77609101e-07\n",
            "  1.12579029e-06  3.70026801e-07], B intercept: -0.6486155040608285, Train loss: 0.27664785999089964, Test loss: 0.29468935757825554\n",
            "\n",
            "-- Epoch no(iteration no)  62 \n",
            " Train data set : \n",
            "W intercept: [-7.39249582e-02  9.17056579e-07  1.27468959e-06  1.23296617e-06\n",
            "  1.06515861e-06  7.37046606e-07  1.55454317e-06  8.95851997e-07\n",
            "  1.14731696e-06  3.72923355e-07], B intercept: -0.6540158430408035, Train loss: 0.27662991638977796, Test loss: 0.29482188094720496\n",
            "\n",
            "-- Epoch no(iteration no)  63 \n",
            " Train data set : \n",
            "W intercept: [-7.47330382e-02  9.34460037e-07  1.29799250e-06  1.25230819e-06\n",
            "  1.09015780e-06  7.52383219e-07  1.58273263e-06  9.15365426e-07\n",
            "  1.16959451e-06  3.83463267e-07], B intercept: -0.6590184706283688, Train loss: 0.27661575257692805, Test loss: 0.29494734517395504\n",
            "\n",
            "-- Epoch no(iteration no)  64 \n",
            " Train data set : \n",
            "W intercept: [-7.58024391e-02  9.51840598e-07  1.32591319e-06  1.27605789e-06\n",
            "  1.11119621e-06  7.71381503e-07  1.61283722e-06  9.29177332e-07\n",
            "  1.19016224e-06  3.91800035e-07], B intercept: -0.6641505827093377, Train loss: 0.27660363846274255, Test loss: 0.29507882434165295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 98/100 [00:00<00:00, 159.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Epoch no(iteration no)  65 \n",
            " Train data set : \n",
            "W intercept: [-7.29514113e-02  9.84627404e-07  1.35444109e-06  1.30972919e-06\n",
            "  1.13387604e-06  7.94892873e-07  1.66456188e-06  9.54942901e-07\n",
            "  1.21989419e-06  3.97160978e-07], B intercept: -0.6202180735642151, Train loss: 0.2767864853022428, Test loss: 0.2940416233749219\n",
            "\n",
            "-- Epoch no(iteration no)  66 \n",
            " Train data set : \n",
            "W intercept: [-7.30329309e-02  9.98989998e-07  1.37456214e-06  1.32978558e-06\n",
            "  1.14962033e-06  8.02557531e-07  1.68665208e-06  9.67782720e-07\n",
            "  1.23409659e-06  3.97874943e-07], B intercept: -0.62705505321253, Train loss: 0.27674584307870237, Test loss: 0.2941903257166827\n",
            "\n",
            "-- Epoch no(iteration no)  67 \n",
            " Train data set : \n",
            "W intercept: [-7.49333601e-02  1.01307135e-06  1.39128890e-06  1.34920544e-06\n",
            "  1.16259779e-06  8.15029669e-07  1.71142311e-06  9.77888955e-07\n",
            "  1.24889646e-06  4.04677073e-07], B intercept: -0.6435180068569908, Train loss: 0.2766653025806183, Test loss: 0.2945681721446951\n",
            "\n",
            "-- Epoch no(iteration no)  68 \n",
            " Train data set : \n",
            "W intercept: [-7.63459835e-02  1.03127924e-06  1.40599958e-06  1.36583982e-06\n",
            "  1.17358162e-06  8.22260723e-07  1.73253073e-06  9.85865436e-07\n",
            "  1.26530747e-06  4.09564799e-07], B intercept: -0.6585691633752684, Train loss: 0.2766155731289527, Test loss: 0.2949370658852339\n",
            "\n",
            "-- Epoch no(iteration no)  69 \n",
            " Train data set : \n",
            "W intercept: [-7.94015871e-02  1.04355184e-06  1.42402511e-06  1.38065920e-06\n",
            "  1.18863296e-06  8.33827017e-07  1.74730951e-06  9.94202705e-07\n",
            "  1.27939973e-06  4.17960764e-07], B intercept: -0.6825523787524647, Train loss: 0.276582071895503, Test loss: 0.2955718153135369\n",
            "\n",
            "-- Epoch no(iteration no)  70 \n",
            " Train data set : \n",
            "W intercept: [-8.16293312e-02  1.05610627e-06  1.44528506e-06  1.40239665e-06\n",
            "  1.20668946e-06  8.49122733e-07  1.77500073e-06  1.00900416e-06\n",
            "  1.29719824e-06  4.28358275e-07], B intercept: -0.6862546454676258, Train loss: 0.27658093953571294, Test loss: 0.2956764059048451\n",
            "\n",
            "-- Epoch no(iteration no)  71 \n",
            " Train data set : \n",
            "W intercept: [-8.07130256e-02  1.08036113e-06  1.47178574e-06  1.42047468e-06\n",
            "  1.22650325e-06  8.63966109e-07  1.80003801e-06  1.02237114e-06\n",
            "  1.31725202e-06  4.32319959e-07], B intercept: -0.6697184156271062, Train loss: 0.27659081740271746, Test loss: 0.29522724134114303\n",
            "\n",
            "-- Epoch no(iteration no)  72 \n",
            " Train data set : \n",
            "W intercept: [-8.30313531e-02  1.08998067e-06  1.47922939e-06  1.43014181e-06\n",
            "  1.23613795e-06  8.70613962e-07  1.82330337e-06  1.03273644e-06\n",
            "  1.32862335e-06  4.35294895e-07], B intercept: -0.6933702975424363, Train loss: 0.2765843587311681, Test loss: 0.29587882943682214\n",
            "\n",
            "-- Epoch no(iteration no)  73 \n",
            " Train data set : \n",
            "W intercept: [-8.37108837e-02  1.10924702e-06  1.50413700e-06  1.45032658e-06\n",
            "  1.25431780e-06  8.86589781e-07  1.84697706e-06  1.05418533e-06\n",
            "  1.34754629e-06  4.42515895e-07], B intercept: -0.6961321511626776, Train loss: 0.2765869726953783, Test loss: 0.29595888426412087\n",
            "\n",
            "-- Epoch no(iteration no)  74 \n",
            " Train data set : \n",
            "W intercept: [-8.33729674e-02  1.13010799e-06  1.52152459e-06  1.46895852e-06\n",
            "  1.27170135e-06  8.98751502e-07  1.86608219e-06  1.06735370e-06\n",
            "  1.37064462e-06  4.50447511e-07], B intercept: -0.6994401480116075, Train loss: 0.2765916618163756, Test loss: 0.2960546428883051\n",
            "\n",
            "-- Epoch no(iteration no)  75 \n",
            " Train data set : \n",
            "W intercept: [-8.09772404e-02  1.15955362e-06  1.56137164e-06  1.50755919e-06\n",
            "  1.29842448e-06  9.18523245e-07  1.90893933e-06  1.09803562e-06\n",
            "  1.40185703e-06  4.54394581e-07], B intercept: -0.6630029027515122, Train loss: 0.2766019793018385, Test loss: 0.2950529078490379\n",
            "\n",
            "-- Epoch no(iteration no)  76 \n",
            " Train data set : \n",
            "W intercept: [-8.24628980e-02  1.17028464e-06  1.58481001e-06  1.53172544e-06\n",
            "  1.31372535e-06  9.32536692e-07  1.93238588e-06  1.11079660e-06\n",
            "  1.41690962e-06  4.55377665e-07], B intercept: -0.677135968308491, Train loss: 0.2765822094785383, Test loss: 0.29542649255024306\n",
            "\n",
            "-- Epoch no(iteration no)  77 \n",
            " Train data set : \n",
            "W intercept: [-8.38719563e-02  1.18149592e-06  1.60047325e-06  1.54466068e-06\n",
            "  1.32864419e-06  9.36783028e-07  1.95153315e-06  1.12770114e-06\n",
            "  1.43678245e-06  4.58365859e-07], B intercept: -0.6902306941719701, Train loss: 0.2765814237529183, Test loss: 0.29579022013390077\n",
            "\n",
            "-- Epoch no(iteration no)  78 \n",
            " Train data set : \n",
            "W intercept: [-8.45664167e-02  1.19224646e-06  1.62211937e-06  1.56580738e-06\n",
            "  1.34505924e-06  9.47007473e-07  1.97837160e-06  1.14687325e-06\n",
            "  1.45742261e-06  4.58617857e-07], B intercept: -0.6932091868044364, Train loss: 0.2765833968371702, Test loss: 0.2958756528772207\n",
            "\n",
            "-- Epoch no(iteration no)  79 \n",
            " Train data set : \n",
            "W intercept: [-8.31722200e-02  1.20954009e-06  1.64704468e-06  1.59021626e-06\n",
            "  1.36796714e-06  9.67961357e-07  2.00874951e-06  1.16808376e-06\n",
            "  1.48681908e-06  4.65083767e-07], B intercept: -0.6859130415389613, Train loss: 0.27657995906523225, Test loss: 0.2956682220304842\n",
            "\n",
            "-- Epoch no(iteration no)  80 \n",
            " Train data set : \n",
            "W intercept: [-8.48315302e-02  1.22334552e-06  1.66153258e-06  1.60469579e-06\n",
            "  1.38792596e-06  9.77267419e-07  2.02696126e-06  1.17715969e-06\n",
            "  1.49183208e-06  4.76054534e-07], B intercept: -0.7085915246331005, Train loss: 0.2766090342223365, Test loss: 0.2963274606297416\n",
            "\n",
            "-- Epoch no(iteration no)  81 \n",
            " Train data set : \n",
            "W intercept: [-8.21130703e-02  1.25803843e-06  1.69288048e-06  1.63704901e-06\n",
            "  1.40954647e-06  9.95209304e-07  2.07100707e-06  1.19810275e-06\n",
            "  1.52096320e-06  4.98945507e-07], B intercept: -0.6711901146982016, Train loss: 0.27658793328873243, Test loss: 0.29526712903674684\n",
            "\n",
            "-- Epoch no(iteration no)  82 \n",
            " Train data set : \n",
            "W intercept: [-8.20593982e-02  1.27428955e-06  1.71670542e-06  1.66235249e-06\n",
            "  1.42843662e-06  1.00735939e-06  2.09260320e-06  1.21738700e-06\n",
            "  1.54625960e-06  5.05823743e-07], B intercept: -0.6659706245455668, Train loss: 0.27659562332349785, Test loss: 0.2951302684411483\n",
            "\n",
            "-- Epoch no(iteration no)  83 \n",
            " Train data set : \n",
            "W intercept: [-8.30168113e-02  1.27924215e-06  1.73985107e-06  1.68301651e-06\n",
            "  1.45748479e-06  1.02278964e-06  2.12237014e-06  1.23677969e-06\n",
            "  1.56324583e-06  5.14170923e-07], B intercept: -0.6704550302671051, Train loss: 0.2765882145015827, Test loss: 0.295248378479572\n",
            "\n",
            "-- Epoch no(iteration no)  84 \n",
            " Train data set : \n",
            "W intercept: [-8.27745690e-02  1.30767410e-06  1.76961316e-06  1.70734865e-06\n",
            "  1.48036642e-06  1.04092431e-06  2.15385508e-06  1.25565571e-06\n",
            "  1.58808307e-06  5.20568471e-07], B intercept: -0.6649891336309925, Train loss: 0.2765968242647947, Test loss: 0.2951053416008258\n",
            "\n",
            "-- Epoch no(iteration no)  85 \n",
            " Train data set : \n",
            "W intercept: [-8.25089597e-02  1.32723609e-06  1.79471669e-06  1.73639169e-06\n",
            "  1.50177831e-06  1.05859869e-06  2.19420934e-06  1.27576816e-06\n",
            "  1.60852042e-06  5.26730718e-07], B intercept: -0.6493058176745311, Train loss: 0.2766374067255566, Test loss: 0.2947114290153297\n",
            "\n",
            "-- Epoch no(iteration no)  86 \n",
            " Train data set : \n",
            "W intercept: [-8.11283619e-02  1.34819641e-06  1.81246194e-06  1.75481809e-06\n",
            "  1.51614880e-06  1.07541523e-06  2.22686688e-06  1.29070626e-06\n",
            "  1.62626517e-06  5.29891213e-07], B intercept: -0.6356343573700223, Train loss: 0.27669355241894217, Test loss: 0.2943871576275868\n",
            "\n",
            "-- Epoch no(iteration no)  87 \n",
            " Train data set : \n",
            "W intercept: [-8.12549854e-02  1.36530880e-06  1.84025703e-06  1.78187064e-06\n",
            "  1.53628235e-06  1.08838035e-06  2.25245742e-06  1.30826875e-06\n",
            "  1.65011682e-06  5.36227037e-07], B intercept: -0.6417846696875231, Train loss: 0.2766664515122386, Test loss: 0.2945304768160441\n",
            "\n",
            "-- Epoch no(iteration no)  88 \n",
            " Train data set : \n",
            "W intercept: [-8.52672949e-02  1.37263895e-06  1.85026940e-06  1.79529331e-06\n",
            "  1.54573460e-06  1.09438344e-06  2.27251454e-06  1.31396424e-06\n",
            "  1.66454646e-06  5.40626615e-07], B intercept: -0.6769036149731248, Train loss: 0.27658052636541747, Test loss: 0.2954224655305136\n",
            "\n",
            "-- Epoch no(iteration no)  89 \n",
            " Train data set : \n",
            "W intercept: [-8.58652846e-02  1.39577963e-06  1.86805116e-06  1.81257591e-06\n",
            "  1.56203520e-06  1.10471602e-06  2.29051299e-06  1.32329903e-06\n",
            "  1.67984612e-06  5.48531445e-07], B intercept: -0.680961488701908, Train loss: 0.27657837666427565, Test loss: 0.29553351177298226\n",
            "\n",
            "-- Epoch no(iteration no)  90 \n",
            " Train data set : \n",
            "W intercept: [-8.67662119e-02  1.41167252e-06  1.89385143e-06  1.83812332e-06\n",
            "  1.57741830e-06  1.12332161e-06  2.31654829e-06  1.33793390e-06\n",
            "  1.69994361e-06  5.56968221e-07], B intercept: -0.6842488818108813, Train loss: 0.2765775819581703, Test loss: 0.2956250296904354\n",
            "\n",
            "-- Epoch no(iteration no)  91 \n",
            " Train data set : \n",
            "W intercept: [-9.01573669e-02  1.42095361e-06  1.90826288e-06  1.84757848e-06\n",
            "  1.59407057e-06  1.12932443e-06  2.33669687e-06  1.34368376e-06\n",
            "  1.71137621e-06  5.63928620e-07], B intercept: -0.6971611811498235, Train loss: 0.27658506637260616, Test loss: 0.2959949050424545\n",
            "\n",
            "-- Epoch no(iteration no)  92 \n",
            " Train data set : \n",
            "W intercept: [-9.04721891e-02  1.44152184e-06  1.92563731e-06  1.86246009e-06\n",
            "  1.61017829e-06  1.14320121e-06  2.35875734e-06  1.35879337e-06\n",
            "  1.73218278e-06  5.65910541e-07], B intercept: -0.7005575902842168, Train loss: 0.2765900832627755, Test loss: 0.2960944018251952\n",
            "\n",
            "-- Epoch no(iteration no)  93 \n",
            " Train data set : \n",
            "W intercept: [-9.09937200e-02  1.46033244e-06  1.94636201e-06  1.88268287e-06\n",
            "  1.62241271e-06  1.15871723e-06  2.38421323e-06  1.37455187e-06\n",
            "  1.74517581e-06  5.71655212e-07], B intercept: -0.7031774287904509, Train loss: 0.2765945962531917, Test loss: 0.29617220204279593\n",
            "\n",
            "-- Epoch no(iteration no)  94 \n",
            " Train data set : \n",
            "W intercept: [-8.85919243e-02  1.48960447e-06  1.97378324e-06  1.90736994e-06\n",
            "  1.64932801e-06  1.17620165e-06  2.41858912e-06  1.39875878e-06\n",
            "  1.77407721e-06  5.83594956e-07], B intercept: -0.6757317555398753, Train loss: 0.2765791407911727, Test loss: 0.29539349186533503\n",
            "\n",
            "-- Epoch no(iteration no)  95 \n",
            " Train data set : \n",
            "W intercept: [-8.96867640e-02  1.50244354e-06  1.99300968e-06  1.92758754e-06\n",
            "  1.66255213e-06  1.18322666e-06  2.44251741e-06  1.41353574e-06\n",
            "  1.79429125e-06  5.84590347e-07], B intercept: -0.6884358265977304, Train loss: 0.2765771509249218, Test loss: 0.29574476884143563\n",
            "\n",
            "-- Epoch no(iteration no)  96 \n",
            " Train data set : \n",
            "W intercept: [-9.02181853e-02  1.52208310e-06  2.02510187e-06  1.95843050e-06\n",
            "  1.68255043e-06  1.20293853e-06  2.46993441e-06  1.43247827e-06\n",
            "  1.81600897e-06  5.88043170e-07], B intercept: -0.6825030600140275, Train loss: 0.27657548007166416, Test loss: 0.2955796467828774\n",
            "\n",
            "-- Epoch no(iteration no)  97 \n",
            " Train data set : \n",
            "W intercept: [-9.13271185e-02  1.53278930e-06  2.04299207e-06  1.97457115e-06\n",
            "  1.69820888e-06  1.21214705e-06  2.49477626e-06  1.44939635e-06\n",
            "  1.83713993e-06  5.89535484e-07], B intercept: -0.6955919660321312, Train loss: 0.27658246303485556, Test loss: 0.29595057851636486\n",
            "\n",
            "-- Epoch no(iteration no)  98 \n",
            " Train data set : \n",
            "W intercept: [-9.10914909e-02  1.55156807e-06  2.06542785e-06  1.99802537e-06\n",
            "  1.71889148e-06  1.22216534e-06  2.52569106e-06  1.47060213e-06\n",
            "  1.86129644e-06  5.91288867e-07], B intercept: -0.6881755112471154, Train loss: 0.2765762299961593, Test loss: 0.29573869838384614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 100/100 [00:00<00:00, 154.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Epoch no(iteration no)  99 \n",
            " Train data set : \n",
            "W intercept: [-9.17354959e-02  1.55966716e-06  2.08762918e-06  2.01945852e-06\n",
            "  1.73335758e-06  1.23593857e-06  2.54412354e-06  1.48810752e-06\n",
            "  1.87902353e-06  5.93788025e-07], B intercept: -0.7008550843313621, Train loss: 0.2765900008925568, Test loss: 0.2961044090813039\n",
            "\n",
            "-- Epoch no(iteration no)  100 \n",
            " Train data set : \n",
            "W intercept: [-9.23472031e-02  1.57031076e-06  2.10265562e-06  2.03396607e-06\n",
            "  1.75034595e-06  1.24699496e-06  2.56684185e-06  1.50041116e-06\n",
            "  1.89726055e-06  6.03261769e-07], B intercept: -0.7133291294254334, Train loss: 0.27661874308279455, Test loss: 0.2964793574387078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
        "clf\n",
        "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
        "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
        "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
        "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
        "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
        "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)\n",
        "clf.fit(X=X_train, y=y_train)"
      ],
      "metadata": {
        "id": "1JbW9eh7KVu4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94bdd1bf-fefc-4162-fbf1-7fff1e6e1fda"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 0.00, NNZs: 10, Bias: -0.008446, T: 512, Avg. loss: 0.692455\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.00, NNZs: 10, Bias: -0.016788, T: 1024, Avg. loss: 0.691068\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.00, NNZs: 10, Bias: -0.025021, T: 1536, Avg. loss: 0.689717\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.00, NNZs: 10, Bias: -0.033147, T: 2048, Avg. loss: 0.688401\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.00, NNZs: 10, Bias: -0.041169, T: 2560, Avg. loss: 0.687117\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.00, NNZs: 10, Bias: -0.049083, T: 3072, Avg. loss: 0.685868\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.01, NNZs: 10, Bias: -0.056905, T: 3584, Avg. loss: 0.684649\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.01, NNZs: 10, Bias: -0.064621, T: 4096, Avg. loss: 0.683461\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.01, NNZs: 10, Bias: -0.072239, T: 4608, Avg. loss: 0.682304\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.01, NNZs: 10, Bias: -0.079753, T: 5120, Avg. loss: 0.681177\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.01, NNZs: 10, Bias: -0.087174, T: 5632, Avg. loss: 0.680078\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.01, NNZs: 10, Bias: -0.094504, T: 6144, Avg. loss: 0.679007\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.01, NNZs: 10, Bias: -0.101742, T: 6656, Avg. loss: 0.677963\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 0.01, NNZs: 10, Bias: -0.108885, T: 7168, Avg. loss: 0.676945\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 0.01, NNZs: 10, Bias: -0.115932, T: 7680, Avg. loss: 0.675953\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 0.01, NNZs: 10, Bias: -0.122899, T: 8192, Avg. loss: 0.674987\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 0.01, NNZs: 10, Bias: -0.129779, T: 8704, Avg. loss: 0.674044\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 0.01, NNZs: 10, Bias: -0.136568, T: 9216, Avg. loss: 0.673124\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 0.01, NNZs: 10, Bias: -0.143269, T: 9728, Avg. loss: 0.672229\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 19 epochs took 0.02 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
              "              random_state=15, verbose=2)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w-clf.coef_, b-clf.intercept_"
      ],
      "metadata": {
        "id": "W9Ace8j3OF-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15ae2b39-4201-44bd-eef7-88bbb4f01032"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-7.96310314e-02,  1.53866116e-06,  2.03881541e-06,\n",
              "          1.97309033e-06,  1.69780127e-06,  1.21722840e-06,\n",
              "          2.48877180e-06,  1.46283667e-06,  1.83933911e-06,\n",
              "          5.84057254e-07]]), array([-0.57006024]))"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict Function"
      ],
      "metadata": {
        "id": "WRvSQDV8eVRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pred(w,b, X):\n",
        "    N = len(X)\n",
        "    predict = []\n",
        "    for i in range(N):\n",
        "        z=np.dot(w,X[i])+b\n",
        "        # Idk why the confidence interval is messed up \n",
        "        if sigmoid(z) >= 0.5: \n",
        "            predict.append(1)\n",
        "        else:\n",
        "            predict.append(0)\n",
        "    return np.array(predict)\n",
        "\n",
        "print(1-np.sum(y_train - pred(w,b,X_train))/len(X_train))\n",
        "print(1-np.sum(y_test  - pred(w,b,X_test))/len(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w3wDqyjOn6R",
        "outputId": "bf06a9bf-4ae0-4eb5-8b1c-c91e5ab66dc6"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.666015625\n",
            "0.6023391812865497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wgrBIrVEQ7Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = np.arange(1, epochs+1, 1)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, train_loss, label='Train Loss')\n",
        "plt.plot(epochs, test_loss, label='Test Loss')\n",
        "plt.title('Epoch vs Train,Test Loss')\n",
        "plt.xlabel(\"Epoch_no\")\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "print(100*'==')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "OG9PuXvoQ82b",
        "outputId": "1ee681fc-6b46-404e-ea4d-b8f0a1bb992e"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================================================================================================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}