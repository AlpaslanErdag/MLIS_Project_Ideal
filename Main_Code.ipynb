{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main_Code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOAbH6oHClGu3lXwd4/LnlH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anshuman-37/MLIS_Project_Ideal/blob/main/Main_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the main jupyter notebook for the coursework of MLIS \n",
        "## Grp 4B\n",
        "## Authors\n",
        "## Alpaslan Erdag , Anshuman Singh , Yixin Fan\n",
        "## Date - 15/01/2022"
      ],
      "metadata": {
        "id": "e6rrsFyi4Bd9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> <center> Abstract </center> </h1>\n",
        "â€‹<em>\n",
        "\n",
        "Todo - Write some abstract about the data giving some information about.\n",
        "\n",
        "The data is obtanined from UCI repository which can be obtained from the [here](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28original%29)\n",
        "</em>"
      ],
      "metadata": {
        "id": "G-IY5zC537OU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Also should we show this ? tell me your thoughts\n",
        "# about_data ='/Users/anshuman/Desktop/Project_Folder/MLIS_Project_Ideal/breast-cancer-wisconsin.names'\n",
        "about_data = '/content/breast-cancer-wisconsin.names'\n",
        "with open(about_data) as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3boaTk7A4FrC",
        "outputId": "28e07a26-0a3f-4422-b507-f887576fdaca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Citation Request:\n",
            "   This breast cancer databases was obtained from the University of Wisconsin\n",
            "   Hospitals, Madison from Dr. William H. Wolberg.  If you publish results\n",
            "   when using this database, then please include this information in your\n",
            "   acknowledgements.  Also, please cite one or more of:\n",
            "\n",
            "   1. O. L. Mangasarian and W. H. Wolberg: \"Cancer diagnosis via linear \n",
            "      programming\", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.\n",
            "\n",
            "   2. William H. Wolberg and O.L. Mangasarian: \"Multisurface method of \n",
            "      pattern separation for medical diagnosis applied to breast cytology\", \n",
            "      Proceedings of the National Academy of Sciences, U.S.A., Volume 87, \n",
            "      December 1990, pp 9193-9196.\n",
            "\n",
            "   3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg: \"Pattern recognition \n",
            "      via linear programming: Theory and application to medical diagnosis\", \n",
            "      in: \"Large-scale numerical optimization\", Thomas F. Coleman and Yuying\n",
            "      Li, editors, SIAM Publications, Philadelphia 1990, pp 22-30.\n",
            "\n",
            "   4. K. P. Bennett & O. L. Mangasarian: \"Robust linear programming \n",
            "      discrimination of two linearly inseparable sets\", Optimization Methods\n",
            "      and Software 1, 1992, 23-34 (Gordon & Breach Science Publishers).\n",
            "\n",
            "1. Title: Wisconsin Breast Cancer Database (January 8, 1991)\n",
            "\n",
            "2. Sources:\n",
            "   -- Dr. WIlliam H. Wolberg (physician)\n",
            "      University of Wisconsin Hospitals\n",
            "      Madison, Wisconsin\n",
            "      USA\n",
            "   -- Donor: Olvi Mangasarian (mangasarian@cs.wisc.edu)\n",
            "      Received by David W. Aha (aha@cs.jhu.edu)\n",
            "   -- Date: 15 July 1992\n",
            "\n",
            "3. Past Usage:\n",
            "\n",
            "   Attributes 2 through 10 have been used to represent instances.\n",
            "   Each instance has one of 2 possible classes: benign or malignant.\n",
            "\n",
            "   1. Wolberg,~W.~H., \\& Mangasarian,~O.~L. (1990). Multisurface method of \n",
            "      pattern separation for medical diagnosis applied to breast cytology. In\n",
            "      {\\it Proceedings of the National Academy of Sciences}, {\\it 87},\n",
            "      9193--9196.\n",
            "      -- Size of data set: only 369 instances (at that point in time)\n",
            "      -- Collected classification results: 1 trial only\n",
            "      -- Two pairs of parallel hyperplanes were found to be consistent with\n",
            "         50% of the data\n",
            "         -- Accuracy on remaining 50% of dataset: 93.5%\n",
            "      -- Three pairs of parallel hyperplanes were found to be consistent with\n",
            "         67% of data\n",
            "         -- Accuracy on remaining 33% of dataset: 95.9%\n",
            "\n",
            "   2. Zhang,~J. (1992). Selecting typical instances in instance-based\n",
            "      learning.  In {\\it Proceedings of the Ninth International Machine\n",
            "      Learning Conference} (pp. 470--479).  Aberdeen, Scotland: Morgan\n",
            "      Kaufmann.\n",
            "      -- Size of data set: only 369 instances (at that point in time)\n",
            "      -- Applied 4 instance-based learning algorithms \n",
            "      -- Collected classification results averaged over 10 trials\n",
            "      -- Best accuracy result: \n",
            "         -- 1-nearest neighbor: 93.7%\n",
            "         -- trained on 200 instances, tested on the other 169\n",
            "      -- Also of interest:\n",
            "         -- Using only typical instances: 92.2% (storing only 23.1 instances)\n",
            "         -- trained on 200 instances, tested on the other 169\n",
            "\n",
            "4. Relevant Information:\n",
            "\n",
            "   Samples arrive periodically as Dr. Wolberg reports his clinical cases.\n",
            "   The database therefore reflects this chronological grouping of the data.\n",
            "   This grouping information appears immediately below, having been removed\n",
            "   from the data itself:\n",
            "\n",
            "     Group 1: 367 instances (January 1989)\n",
            "     Group 2:  70 instances (October 1989)\n",
            "     Group 3:  31 instances (February 1990)\n",
            "     Group 4:  17 instances (April 1990)\n",
            "     Group 5:  48 instances (August 1990)\n",
            "     Group 6:  49 instances (Updated January 1991)\n",
            "     Group 7:  31 instances (June 1991)\n",
            "     Group 8:  86 instances (November 1991)\n",
            "     -----------------------------------------\n",
            "     Total:   699 points (as of the donated datbase on 15 July 1992)\n",
            "\n",
            "   Note that the results summarized above in Past Usage refer to a dataset\n",
            "   of size 369, while Group 1 has only 367 instances.  This is because it\n",
            "   originally contained 369 instances; 2 were removed.  The following\n",
            "   statements summarizes changes to the original Group 1's set of data:\n",
            "\n",
            "   #####  Group 1 : 367 points: 200B 167M (January 1989)\n",
            "   #####  Revised Jan 10, 1991: Replaced zero bare nuclei in 1080185 & 1187805\n",
            "   #####  Revised Nov 22,1991: Removed 765878,4,5,9,7,10,10,10,3,8,1 no record\n",
            "   #####                  : Removed 484201,2,7,8,8,4,3,10,3,4,1 zero epithelial\n",
            "   #####                  : Changed 0 to 1 in field 6 of sample 1219406\n",
            "   #####                  : Changed 0 to 1 in field 8 of following sample:\n",
            "   #####                  : 1182404,2,3,1,1,1,2,0,1,1,1\n",
            "\n",
            "5. Number of Instances: 699 (as of 15 July 1992)\n",
            "\n",
            "6. Number of Attributes: 10 plus the class attribute\n",
            "\n",
            "7. Attribute Information: (class attribute has been moved to last column)\n",
            "\n",
            "   #  Attribute                     Domain\n",
            "   -- -----------------------------------------\n",
            "   1. Sample code number            id number\n",
            "   2. Clump Thickness               1 - 10\n",
            "   3. Uniformity of Cell Size       1 - 10\n",
            "   4. Uniformity of Cell Shape      1 - 10\n",
            "   5. Marginal Adhesion             1 - 10\n",
            "   6. Single Epithelial Cell Size   1 - 10\n",
            "   7. Bare Nuclei                   1 - 10\n",
            "   8. Bland Chromatin               1 - 10\n",
            "   9. Normal Nucleoli               1 - 10\n",
            "  10. Mitoses                       1 - 10\n",
            "  11. Class:                        (2 for benign, 4 for malignant)\n",
            "\n",
            "8. Missing attribute values: 16\n",
            "\n",
            "   There are 16 instances in Groups 1 to 6 that contain a single missing \n",
            "   (i.e., unavailable) attribute value, now denoted by \"?\".  \n",
            "\n",
            "9. Class distribution:\n",
            " \n",
            "   Benign: 458 (65.5%)\n",
            "   Malignant: 241 (34.5%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center>About code (Suggest a good heading)</center></h1>\n",
        "\n",
        "<h2><em>The code is divided in the following modules/parts/chapters?</h2>\n",
        "<ol>\n",
        "<li> <h4> Data Cleaning and Data Preprocessing </h4></li> \n",
        "<li> <h4>Model Fiting on the clean data </h4></li> \n",
        "<li> <h4>Model Accuracy </h4></li>\n",
        "</ol>\n",
        "</em>"
      ],
      "metadata": {
        "id": "ZN6gh3N24R-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><center>1. Data Cleaning and Data Preprocessing</center></h2>\n",
        "\n",
        "<p>To get a good result over model accuracies we are going to first clean the data and then used the cleaned and preprocessed data to train a model. </p>\n",
        "\n",
        "<p> We will be using a systematic approach to clean our data. That will be listed in the following steps</p>\n",
        "<em>\n",
        "&emsp; <li>Data Visulaization</li>\n",
        "&emsp; <li>Checking and Removing nan values</li>\n",
        "&emsp; <li>Removing Useless Attributes</li>\n",
        "&emsp; <li>Deleting Outliers and Data Normalizations</li>\n",
        "</em>"
      ],
      "metadata": {
        "id": "0TONMQnQ4SDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>1.1 Data Visualization </h3>"
      ],
      "metadata": {
        "id": "-daRoybV4SJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>1.1.1 Code </h4>"
      ],
      "metadata": {
        "id": "dRQhoFvkfl-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Header Files \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('ggplot')\n",
        "import scipy\n",
        "\n",
        "## Loading Data \n",
        "#data = '/Users/anshuman/Desktop/Project_Folder/MLIS_Project_Ideal/breast-cancer-wisconsin.data'\n",
        "data = '/content/breast-cancer-wisconsin.data'"
      ],
      "metadata": {
        "id": "sK9S6Bky4RlC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_name = ['Samplecodenumber','ClumpThickness','UniformityofCellSize','UniformityofCellShape',\n",
        "            'MarginalAdhesion','SingleEpithelialCellSize','BareNuclei',\n",
        "            'BlandChromatin','NormalNucleoli','Mitoses','Class']\n",
        "cancerdata = pd.read_csv(data,low_memory=False,names=col_name)\n",
        "cancerdata.head()"
      ],
      "metadata": {
        "id": "xL5c7NTs4omD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Removing attribute that is of no use\n",
        "cancerdata = cancerdata.drop(['Samplecodenumber'], 1)"
      ],
      "metadata": {
        "id": "oOGBeeDS4opZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The number of samples present by class count\n",
        "cancerdata['Class'].value_counts()"
      ],
      "metadata": {
        "id": "nV7-qayy4orv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making the class counts to similar size\n",
        "Benign = cancerdata[(cancerdata.Class == 2) ].sample(240).index\n",
        "Malignant = cancerdata[(cancerdata.Class == 4) ].sample(240).index\n",
        "cancer = cancerdata.loc[Benign|Malignant]\n",
        "cancer = cancer.reset_index(drop=True)\n",
        "\n",
        "# Making the classess from 2/4 to 0/1 \n",
        "cancer['classes'] = cancer.Class.map({2:0,4:1})\n",
        "cancer = cancer.drop(['Class'], 1)\n",
        "\n",
        "# Printing the updated dataframe\n",
        "cancer.head()"
      ],
      "metadata": {
        "id": "iY_4Kw5F4ous"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirming that counts of the classes are similar \n",
        "cancer['classes'].value_counts()\n"
      ],
      "metadata": {
        "id": "2gLfIN6F4oxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot date variables and find the variables with similar distributions, keep only one of them.\n",
        "#['ClumpThickness','UniformityofCellSize','UniformityofCellShape','MarginalAdhesion',\n",
        "#'SingleEpithelialCellSize','BareNuclei','BlandChromatin','NormalNucleoli','Mitoses']\n",
        "fig, axs = plt.subplots(3, 3, figsize=(18,18))\n",
        "axs[0,0].hist(cancer['ClumpThickness'])\n",
        "axs[0,0].set_title(\"ClumpThickness\")\n",
        "axs[0,1].hist(cancer['UniformityofCellSize'])\n",
        "axs[0,1].set_title(\"UniformityofCellSize\")\n",
        "axs[0,2].hist(cancer['UniformityofCellShape'])\n",
        "axs[0,2].set_title(\"UniformityofCellShape\")\n",
        "axs[1,0].hist(cancer['MarginalAdhesion'])\n",
        "axs[1,0].set_title(\"MarginalAdhesion\")\n",
        "axs[1,1].hist(cancer['SingleEpithelialCellSize'])\n",
        "axs[1,1].set_title(\"SingleEpithelialCellSize\")\n",
        "axs[1,2].hist(cancer['BareNuclei'])\n",
        "axs[1,2].set_title(\"BareNuclei\")\n",
        "axs[2,0].hist(cancer['BlandChromatin'])\n",
        "axs[2,0].set_title(\"BlandChromatin\")\n",
        "axs[2,1].hist(cancer['NormalNucleoli'])\n",
        "axs[2,1].set_title(\"NormalNucleoli\")\n",
        "axs[2,2].hist(cancer['Mitoses'])\n",
        "axs[2,2].set_title(\"Mitoses\")\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "FxwBcGnk429U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>1.1.2 Observations </h4>\n",
        "<h4>1.1.3 Approach </h4>"
      ],
      "metadata": {
        "id": "wdpWrB3HWel7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>1.2 Checking and Removing nan values</h3>"
      ],
      "metadata": {
        "id": "fPcTVTlE46vC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>1.2.1 Code </h4>"
      ],
      "metadata": {
        "id": "wbaWQwlpfyif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO \n",
        "## Yixin remove the wasted steps from the code and only keep the most important data cleaning steps\n",
        "cancer.dtypes.value_counts()"
      ],
      "metadata": {
        "id": "dJclAfmh49KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cancer.select_dtypes('object')"
      ],
      "metadata": {
        "id": "3DCa8TIj4_Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cancer = cancer.replace('?' ,np.nan)\n",
        "\n",
        "##Finding the the count and percentage of values that are missing in the dataframe.\n",
        "null1 = pd.DataFrame({'Count': cancer.isnull().sum(), 'Percent': 100*cancer.isnull().sum()/len(cancer)})\n",
        "\n",
        "##printing columns with null count more than 0\n",
        "null1[null1['Count'] > 0]"
      ],
      "metadata": {
        "id": "1CyRzBnb6XnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Filling the columns with nan values with mean of the data\n",
        "cancer[\"BareNuclei\"]=cancer[\"BareNuclei\"].astype(float)\n",
        "cancer=cancer.fillna(cancer.mean())"
      ],
      "metadata": {
        "id": "mQKh6EdQ6Xrk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>1.2.2 Observations </h4>\n",
        "<h4>1.2.3 Approach </h4>"
      ],
      "metadata": {
        "id": "PHIdqxDmWMPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>1.3 Removing Useless Attributes </h3>"
      ],
      "metadata": {
        "id": "uf89AePf6nsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>1.3.1 Code </h4>"
      ],
      "metadata": {
        "id": "aF8wcn9Gf3au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_list1=cancer.corr()['classes'].abs().sort_values(ascending=False)\n",
        "corr_list_new=corr_list1[corr_list1>0.01].index.values.tolist()\n",
        "corr_list1\n",
        "cancer=cancer[corr_list_new]\n",
        "cancer.head(3)"
      ],
      "metadata": {
        "id": "e1xo7hxe6Xv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## To remove outliers first we have to find the correlation between attributes\n",
        "\n",
        "#find the variables with high correlations\n",
        "cor1 = cancer.corr().abs()\n",
        "list1 = cor1.stack().sort_values(ascending=False).drop_duplicates()  \n",
        "high_corr= list1[list1>0.70].index.values.tolist()\n",
        "high_corr.remove(high_corr[0])\n",
        "\n",
        "display(list1)"
      ],
      "metadata": {
        "id": "5MgSXzzR6XzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for variable pairs with high correlation, keep only one of them\n",
        "columnlist=list(cancer.columns)\n",
        "len(high_corr)\n",
        "for i in range(len(high_corr)):\n",
        "    if \"classes\" in high_corr[i]:\n",
        "        columnlist=columnlist\n",
        "    else:\n",
        "        if high_corr[i][0] in columnlist and high_corr[i][1] in columnlist:\n",
        "            columnlist.remove(high_corr[i][1])\n",
        "        else:\n",
        "            columnlist=columnlist\n",
        "cancer_final=cancer[columnlist]"
      ],
      "metadata": {
        "id": "x7OKFrzE6X3D"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Doing a test train split on the data \n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "training_data, test_data = train_test_split(cancer_final,random_state=None, shuffle=True)\n",
        "train_x=training_data.drop(['classes'], 1) \n",
        "train_y=training_data[['classes']]\n",
        "test_x=test_data.drop(['classes'], 1)\n",
        "test_y=test_data[['classes']]"
      ],
      "metadata": {
        "id": "ZJPMB2Tq6pOq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>1.3.2 Observations </h4>\n",
        "\n",
        "\n",
        "<h4>1.3.3 Approach </h4>"
      ],
      "metadata": {
        "id": "P-oNBnzbV4lI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>1.4 Deleting Outliers and Data Normalizations</h3>"
      ],
      "metadata": {
        "id": "6mQKJvN27Bfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>1.4.1 Code </h4>"
      ],
      "metadata": {
        "id": "tOcvGwDUf8fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove outliers\n",
        "cancer_norm = cancer_final[(np.abs(scipy.stats.zscore(cancer_final)) < 3).all(axis=1)]\n",
        "# Zero mean normalisation\n",
        "cancer_norm.iloc[:,1:]=(cancer_norm.iloc[:,1:]-cancer_norm.iloc[:,1:].mean())/cancer_norm.iloc[:,1:].std()"
      ],
      "metadata": {
        "id": "9Z4bemp56pTY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_norm.head()"
      ],
      "metadata": {
        "id": "MQpTYu396pXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split the new dataset\n",
        "training_data_norm, test_data_norm = train_test_split(cancer_norm,random_state=None, shuffle=True)\n",
        "train_x_norm=training_data_norm.drop(['classes'], 1) \n",
        "train_y_norm=training_data_norm[['classes']]\n",
        "test_x_norm=test_data_norm.drop(['classes'], 1)\n",
        "test_y_norm=test_data_norm[['classes']]"
      ],
      "metadata": {
        "id": "Wkh0puJu6pa3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array(train_x_norm)\n",
        "print(x_train.shape)\n",
        "y_train = np.array(train_y_norm)\n",
        "print(y_train.shape)\n",
        "x_test = np.array(test_x_norm)\n",
        "print(x_test.shape)\n",
        "y_test = np.array(test_y_norm)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlkLRQkM6pee",
        "outputId": "8fc8fc33-4eb4-440d-dd77-37a6680cef4c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(343, 7)\n",
            "(343, 1)\n",
            "(115, 7)\n",
            "(115, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>1.4.2 Observations</h4>\n",
        "<!---> Here will add what are the observations of each and every chapter <--->\n",
        "<h4>1.4.3 Approach </h4>\n"
      ],
      "metadata": {
        "id": "uoxu8NW0VeWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>1.5 Conclusion </h3>"
      ],
      "metadata": {
        "id": "ezxSeKxKWqBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><center>2. Model Fiting on the clean data</center></h2>\n",
        "\n",
        "We have a cleaned and a normalized data now we just have to fit a model over the data and make predictions.\n",
        "We are going to implement 2 models that are implemented by us \n",
        "\n",
        "<ol>\n",
        "    <li>SGD Classifier with Log Loss</li>\n",
        "    <li>SVM with RBF kernel</li>\n",
        "</ol>\n"
      ],
      "metadata": {
        "id": "SNNGSB4Vb2CE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>2.1 SGD Classifier with Log Loss</h3>"
      ],
      "metadata": {
        "id": "LGeB6fiFYPuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>2.1.1 Function Call</h4>"
      ],
      "metadata": {
        "id": "3NIP2zzzYi2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## importing self made SGD classifier as Classifier 1 \n",
        "import Sgd_classifier as classfier_1\n",
        "alpha=0.001\n",
        "eta0=0.01\n",
        "N=len(x_train)\n",
        "epochs=75\n",
        "p = 2\n",
        "w,b,train_loss,test_loss=classfier_1.train(x_train,y_train,x_test,y_test,epochs,alpha,eta0,p)"
      ],
      "metadata": {
        "id": "qq2Z7Etj6yun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h>2.1.2 Coding Approach</h4>\n",
        "\n",
        "<!----> Will be going over the coding how it is done and stuff"
      ],
      "metadata": {
        "id": "PNjgLcYRYcIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>2.2 SVM with RBF kernel </h3>"
      ],
      "metadata": {
        "id": "ndwbS6ZqaLr3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>2.2.1 Function Call</h4>"
      ],
      "metadata": {
        "id": "H-Qtsx6LaTN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>2.2.2 Coding Approach</h4>\n",
        "\n",
        "<!----> Will be going over the coding how it is done and stuff"
      ],
      "metadata": {
        "id": "FUu-87k2aXIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><center>3. Model Accuracy</center></h2>\n",
        "<em>\n",
        "Here we are going to talk about the model accuracies by using test and train loss graphs, in addition to Confussion Matrix and some ROC curves\n",
        "</em>"
      ],
      "metadata": {
        "id": "swzuVJ19S1Jr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>3.1 Model 1 SGD Classifier with Log Loss</h3>\n"
      ],
      "metadata": {
        "id": "qPy611dMa8Bu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>3.1.1 Prediction accuracies</h4>"
      ],
      "metadata": {
        "id": "rI7qXtftah1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred = classfier_1.pred(w,b,x_train)\n",
        "y_test_pred = classfier_1.pred(w,b,x_test)\n",
        "\n",
        "y_train=y_train.reshape(y_train_pred.shape)\n",
        "print('Train_Accuracy : {:.3f}'.format(1-np.sum(y_train - y_train_pred)/len(x_train)))\n",
        "y_test=y_test.reshape(y_test_pred.shape)\n",
        "print('Test_Accuracy  : {:.3f}'.format(1-np.sum(y_test  - y_test_pred)/len(x_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "057fGPgc6x3_",
        "outputId": "c841cc8b-3706-45a5-886f-2b89fe684824"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_Accuracy : 0.997\n",
            "Test_Accuracy  : 0.991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>3.1.2 Test and Train Loss over Epochs</h4>\n",
        "We will plot graph against the number of epochs with respect to the test and train loss. This will give us insight whehter our code is performing gradient descent in a correct manner."
      ],
      "metadata": {
        "id": "7pAXgaM_bjzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = np.arange(1, epochs+1, 1)\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.plot(epochs, train_loss, label='Train Loss')\n",
        "plt.plot(epochs, test_loss, label='Test Loss')\n",
        "plt.title('Epoch vs Train,Test Loss')\n",
        "plt.xlabel(\"Epoch_no\")\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "print(125*'=')\n"
      ],
      "metadata": {
        "id": "1gK7yssz073T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>3.1.3 Confusion Matrix </h4>\n",
        "Here well will print the confusion matrix for the Model with its Precision and Recall values"
      ],
      "metadata": {
        "id": "qcN8YgFwb0gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.metrics import confusion_matrix\n",
        "import confusion_matrix as cm\n",
        "cm,fpr,recall = cm.confusion_matrix(y_test, y_test_pred)\n",
        "# Accuracy is\n",
        "AUC_score = (cm[0][0] + cm[1][1])/(cm[0][1]+cm[1][0]+cm[0][0] + cm[1][1])\n",
        "print('Confusion Matrix\\n',cm)\n",
        "print('False Positive Rate :   {:.3f}'.format(fpr))\n",
        "print('True Positive Rate  :   {:.3f}'.format(recall))\n",
        "print('Area Under Curve (AUC): {:.3f}'.format(AUC_score))"
      ],
      "metadata": {
        "id": "dwjOiXQsTjjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><center>Conclusions</center></h2>\n",
        "We were given a task to implement a model that will help classify between M/B(still have to refine these lines).\n",
        "To do this we first normalize and clean the data so that we can get maximum efficiency in our classification model.\n",
        "Then we implement the models that we created using python scripts and make a call back to then in our main IPython notebook.\n",
        "<br> </br>\n",
        "<p>\n",
        "<em>\n",
        "The following are the results obtained for the repesctive implemnted models :-\n",
        "</p>\n",
        "</em>\n",
        "<em>    \n",
        "\n",
        "<br><h4>1. Stochastic Gradient Descent Classifier </h4>\n",
        "<em>\n",
        "    <ol>\n",
        "    <li></li>\n",
        "    <li></li>\n",
        "    <li></li>\n",
        "    </ol>\n",
        "</em>\n",
        "</br>\n",
        "<br><h4>2. Support Vector Machine </h4>\n",
        "<em>\n",
        "    <ol>\n",
        "    <li></li>\n",
        "    <li></li>\n",
        "    <li></li>\n",
        "    </ol>\n",
        "</em>\n",
        "</br>\n",
        "</em>\n",
        "<h2><center>Code's Shortcomings</center></h2>\n",
        "This code works on assumption that #Gradient descents fault... \n",
        "<em>\n",
        "&emsp;<li>Very High Accuracy (Overfitting?) but still high precission and recall? Why?</li>\n",
        "<li>Really Low Data Points </li>\n",
        "<!---> Some shortcommings about the SVMs stuff? What assumptions we make with the SVM stuff?\n"
      ],
      "metadata": {
        "id": "XW2Czt08hhqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><center>References</center></h2>\n"
      ],
      "metadata": {
        "id": "_tB9iqYfg-NA"
      }
    }
  ]
}